{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZsCO9n0uyy"
      },
      "outputs": [],
      "source": [
        "# # IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# # RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "# # %pip install -q kagglehub\n",
        "# import kagglehub\n",
        "\n",
        "# kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8NBecxp0uyz"
      },
      "outputs": [],
      "source": [
        "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# # NOTEBOOK.\n",
        "\n",
        "# superpotato9_dalle_recognition_dataset_path = kagglehub.dataset_download(\n",
        "#     \"superpotato9/dalle-recognition-dataset\"\n",
        "# )\n",
        "# spectrewolf8_random_images_dataset_path = kagglehub.dataset_download(\n",
        "#     \"spectrewolf8/random-images-dataset\"\n",
        "# )\n",
        "\n",
        "# print(\"Data source import complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6287f8b3-8c48-4b8c-b40d-de9e6f9171fc",
        "_uuid": "6404d9ea-4947-4c49-bcdf-bd037dbb504b",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:42:19.994158Z",
          "iopub.status.busy": "2025-01-28T18:42:19.993694Z",
          "iopub.status.idle": "2025-01-28T18:42:52.908469Z",
          "shell.execute_reply": "2025-01-28T18:42:52.90754Z",
          "shell.execute_reply.started": "2025-01-28T18:42:19.994128Z"
        },
        "id": "ghomG8CZ0uy0",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "77b755af-24e2-41f7-8c18-7bec9bc12ba0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install opencv-python --upgrade\n",
        "# !pip install ultralytics --upgrade\n",
        "# !pip install torchmetrics --upgrade\n",
        "# !pip install grad-cam --upgrade\n",
        "# !pip install natsort --upgrade\n",
        "# !pip install Pillow --upgrade\n",
        "# !pip install wandb --upgrade\n",
        "# !pip install lightning --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a3830d64-8d9b-45ff-bfa4-bcc203cb801a",
        "_uuid": "7ace74e4-e87d-4be8-938d-dae9e5cf61d1",
        "collapsed": false,
        "id": "QofR_RSW0uy1",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "### >Restart kernel!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8dfa77bd-a9bc-4f27-bbe0-0d9dc79cc1ce",
        "_uuid": "e3da3516-e71a-4b16-8188-4d5640d607be",
        "execution": {
          "iopub.execute_input": "2025-01-28T17:29:10.80591Z",
          "iopub.status.busy": "2025-01-28T17:29:10.805588Z",
          "iopub.status.idle": "2025-01-28T17:29:19.611566Z",
          "shell.execute_reply": "2025-01-28T17:29:19.610916Z",
          "shell.execute_reply.started": "2025-01-28T17:29:10.805862Z"
        },
        "id": "ZDLOaC1O0uy1",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as tnf\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image, ImageChops\n",
        "import os, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n",
        "\n",
        "# from ultralytics import YOLO\n",
        "import hashlib\n",
        "from natsort import natsorted\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "709192b0-8a06-4b25-951a-37030585360e",
        "_uuid": "1ea37f95-d885-4287-bc78-616ddd0e23fa",
        "collapsed": false,
        "id": "UpAnme1n0uy2",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "075afe5f-eb9b-4725-9193-8ab33f473bf3",
        "_uuid": "d8d9155e-6713-4245-8948-4b3702ba2f63",
        "collapsed": false,
        "id": "3A-YiVo_0uy2",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Generate Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "efb656e5-1394-4d91-888f-37681ba3cc99",
        "_uuid": "fad063e1-254b-4d1e-bbff-b377505fe2c5",
        "execution": {
          "iopub.execute_input": "2025-01-26T21:06:53.547706Z",
          "iopub.status.busy": "2025-01-26T21:06:53.547379Z",
          "iopub.status.idle": "2025-01-26T21:06:56.269677Z",
          "shell.execute_reply": "2025-01-26T21:06:56.268254Z",
          "shell.execute_reply.started": "2025-01-26T21:06:53.547681Z"
        },
        "id": "f886KmeN0uy2",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !rm -rf '{DATASET_DIR}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40c61097-b271-477b-bb1e-8f4193cd3644",
        "_uuid": "5c05330d-0fa4-409d-aa75-9a0cd9755dbf",
        "execution": {
          "iopub.execute_input": "2025-01-26T21:07:05.389216Z",
          "iopub.status.busy": "2025-01-26T21:07:05.388843Z",
          "iopub.status.idle": "2025-01-26T21:10:25.756471Z",
          "shell.execute_reply": "2025-01-26T21:10:25.755263Z",
          "shell.execute_reply.started": "2025-01-26T21:07:05.389186Z"
        },
        "id": "LgUuYP1k0uy3",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# #create directories\n",
        "# !mkdir '{DATASET_DIR}'\n",
        "# !mkdir '{DATASET_DIR}/real'\n",
        "# !mkdir '{DATASET_DIR}fake'\n",
        "# !mkdir '/kaggle/working/models'\n",
        "\n",
        "# # #copy real images\n",
        "# # !cp -r '/kaggle/input/random-images-dataset/random_images_dataset/training/all_images/'* '{DATASET_DIR}/real/'\n",
        "# # !cp -r '/kaggle/input/dalle-recognition-dataset/real/'* '{DATASET_DIR}/real/'\n",
        "# !find '/kaggle/input/random-images-dataset/random_images_dataset/training/all_images/' -type f -print0 | xargs -0 cp -t '{DATASET_DIR}/real/'\n",
        "# !find '/kaggle/input/dalle-recognition-dataset/real/' -type f -print0 | xargs -0 cp -t '{DATASET_DIR}/real/'\n",
        "\n",
        "# # #copy fake images\n",
        "# # !cp -r '/kaggle/input/dalle-recognition-dataset/fakeV2/fake-v2/'* '{DATASET_DIR}/fake/'\n",
        "# !find '/kaggle/input/dalle-recognition-dataset/fakeV2/fake-v2/' -type f -print0 | xargs -0 cp -t '{DATASET_DIR}/fake/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_DIR = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/Datasets/superpotato9_dalle-recognition-dataset(merged_random-images_Human-faces)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fa1e43a-f561-4947-b3c7-f0b1e33558a7",
        "_uuid": "95a2521c-a913-4bbb-8999-9cb469d8a78f",
        "execution": {
          "iopub.execute_input": "2025-01-26T21:40:57.220419Z",
          "iopub.status.busy": "2025-01-26T21:40:57.220075Z",
          "iopub.status.idle": "2025-01-26T21:40:57.900592Z",
          "shell.execute_reply": "2025-01-26T21:40:57.899315Z",
          "shell.execute_reply.started": "2025-01-26T21:40:57.220394Z"
        },
        "id": "s-tDRjOY0uy4",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#remove unnecessary files\n",
        "!rm -rf \"{DATASET_DIR}/real/r-art.txt\"\n",
        "!rm -rf \"{DATASET_DIR}/fake/sort\"\n",
        "!rm -rf \"{DATASET_DIR}/fake/dataset-metadata.json\"\n",
        "!rm -rf \"{DATASET_DIR}/fake/12479.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae9e00d1-ad1e-4525-bcb9-0659e1130c4d",
        "_uuid": "0861cc1c-9958-472c-9e87-ac6b95e309af",
        "execution": {
          "iopub.execute_input": "2025-01-28T17:29:19.613172Z",
          "iopub.status.busy": "2025-01-28T17:29:19.612676Z",
          "iopub.status.idle": "2025-01-28T17:29:19.944919Z",
          "shell.execute_reply": "2025-01-28T17:29:19.944199Z",
          "shell.execute_reply.started": "2025-01-28T17:29:19.61314Z"
        },
        "id": "X1ZFX9zf0uy4",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Print number of files\n",
        "real_files = [\n",
        "    os.path.join(f\"{DATASET_DIR}/real\", f)\n",
        "    for f in os.listdir(f\"{DATASET_DIR}/real\")\n",
        "    # if os.path.isfile(os.path.join(\"{DATASET_DIR}/real\", f))\n",
        "]\n",
        "fake_files = [\n",
        "    os.path.join(f\"{DATASET_DIR}/fake\", f)\n",
        "    for f in os.listdir(f\"{DATASET_DIR}/fake\")\n",
        "    # if os.path.isfile(os.path.join(\"{DATASET_DIR}/fake\", f))\n",
        "]\n",
        "\n",
        "random.shuffle(real_files)\n",
        "random.shuffle(fake_files)\n",
        "print(\"real files:\", real_files[0:5])\n",
        "print(\"fake files:\", fake_files[0:5])\n",
        "print(f\"Number of real images: {len(real_files)}\")\n",
        "print(f\"Number of fake images: {len(fake_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "960e01ac-72f6-4f71-8605-790f9be9a03a",
        "_uuid": "161fbbf9-57c3-4918-bc60-b6d3dc60817c",
        "execution": {
          "iopub.execute_input": "2025-01-28T17:32:22.585068Z",
          "iopub.status.busy": "2025-01-28T17:32:22.584638Z",
          "iopub.status.idle": "2025-01-28T17:33:06.376534Z",
          "shell.execute_reply": "2025-01-28T17:33:06.375809Z",
          "shell.execute_reply.started": "2025-01-28T17:32:22.585037Z"
        },
        "id": "RpVmL4X70uy5",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "def is_valid_image(file_path):\n",
        "    try:\n",
        "        with Image.open(file_path) as img:\n",
        "            # Check image size\n",
        "            if img.width * img.height > 178956970:  # PIL's max pixel limit\n",
        "                print(f\"Oversized image: {file_path}\")\n",
        "                return False\n",
        "\n",
        "            # Additional checks\n",
        "            img.verify()  # Verify the image is not corrupted\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Invalid image {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Modify your dataset loading to skip invalid images\n",
        "valid_image_paths_1 = [path for path in real_files if is_valid_image(path)]\n",
        "valid_image_paths_2 = [path for path in fake_files if is_valid_image(path)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0338e30-c523-4d01-a12c-1abcd1c1da04",
        "_uuid": "5d9ecd4c-7d76-4aab-adb6-52ac97bfa08b",
        "collapsed": false,
        "id": "qCI2N5cZ0uy5",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4810eb2f-6c7a-4e06-98a8-45e51b286800",
        "_uuid": "3535523f-162f-433d-8d03-06c5929aca12",
        "execution": {
          "iopub.execute_input": "2025-01-28T17:33:06.377839Z",
          "iopub.status.busy": "2025-01-28T17:33:06.377589Z",
          "iopub.status.idle": "2025-01-28T17:33:06.382152Z",
          "shell.execute_reply": "2025-01-28T17:33:06.381421Z",
          "shell.execute_reply.started": "2025-01-28T17:33:06.377818Z"
        },
        "id": "2G5MCfBY0uy5",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "052e6e29-e382-48bd-813f-5bd362164497",
        "_uuid": "579b22f0-7110-46d3-a547-fefaf675ca86",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:51:21.642613Z",
          "iopub.status.busy": "2025-01-28T18:51:21.6423Z",
          "iopub.status.idle": "2025-01-28T18:51:21.649136Z",
          "shell.execute_reply": "2025-01-28T18:51:21.648372Z",
          "shell.execute_reply.started": "2025-01-28T18:51:21.64259Z"
        },
        "id": "zqFQ3eDP0uy5",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class AiGenDataset(Dataset):\n",
        "    def __init__(self, image_dir, cap=15000, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.samples_cap = cap\n",
        "\n",
        "        # Assuming there are two folders: 'real' and 'deepfake' in 'dataset' directory\n",
        "        for label in [\"real\", \"fake\"]:\n",
        "            label_dir = os.path.join(image_dir, label)\n",
        "            image_files = os.listdir(label_dir)\n",
        "            random.shuffle(image_files)\n",
        "            # Iterate over all images in 'real' and 'fake' directories\n",
        "            for idx, img_file in enumerate(image_files):\n",
        "                if idx > self.samples_cap:\n",
        "                    break\n",
        "                img_path = os.path.join(label_dir, img_file)\n",
        "                self.image_paths.append(img_path)\n",
        "\n",
        "                # Assign labels: 'real' -> 0, 'fake' -> 1\n",
        "                self.labels.append(0 if label == \"real\" else 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # try:\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image and convert it to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply the transform, if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error loading image {img_path}: {e}\")\n",
        "        #     image = Image.new('RGB', (224, 224), color='black')\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43f70544-3d4e-4e10-899b-670f65ac5bc1",
        "_uuid": "d8da1e5e-c924-4c55-b6d3-4f3a28f7e2b7",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:51:25.964031Z",
          "iopub.status.busy": "2025-01-28T18:51:25.963632Z",
          "iopub.status.idle": "2025-01-28T18:51:26.001078Z",
          "shell.execute_reply": "2025-01-28T18:51:26.000338Z",
          "shell.execute_reply.started": "2025-01-28T18:51:25.963996Z"
        },
        "id": "f4G_4GBK0uy6",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "645933b3-d015-45f7-dbc5-dc3fa5634ca8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# /kaggle/working/dataset/\n",
        "# ├── real/\n",
        "# └── fake/\n",
        "\n",
        "# Define the dataset with all data\n",
        "dataset = AiGenDataset(image_dir=f\"{DATASET_DIR}\", transform=transform)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Print details for confirmation\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Train loader: {train_loader}\")\n",
        "print(f\"Validation loader: {val_loader}\")\n",
        "print(f\"Test loader: {test_loader}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d8d1ea8-adf6-4f87-9262-b084d5d3a5f5",
        "_uuid": "98dc2ac7-7c93-4e60-bb90-549bd0eeda25",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:32:55.924925Z",
          "iopub.status.busy": "2025-01-28T18:32:55.924544Z",
          "iopub.status.idle": "2025-01-28T18:32:57.455433Z",
          "shell.execute_reply": "2025-01-28T18:32:57.454735Z",
          "shell.execute_reply.started": "2025-01-28T18:32:55.92489Z"
        },
        "id": "Jr8lMz_F0uy6",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torchvision.models import ResNeXt101_32X8D_Weights\n",
        "\n",
        "# model = models.resnext50_32x4d()\n",
        "# model = models.resnext101_32x8d()\n",
        "\n",
        "model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.DEFAULT)\n",
        "\n",
        "# Modify the last layer to match the number of classes (real or fake)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: real and fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec515add-5b9d-42d7-9895-0e504425f6e8",
        "_uuid": "a8ba8025-1bba-4f0a-85e3-35184c35f658",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:33:01.178333Z",
          "iopub.status.busy": "2025-01-28T18:33:01.17798Z",
          "iopub.status.idle": "2025-01-28T18:33:01.185248Z",
          "shell.execute_reply": "2025-01-28T18:33:01.184584Z",
          "shell.execute_reply.started": "2025-01-28T18:33:01.178302Z"
        },
        "id": "lEhlc1Vh0uy7",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc95645f-5427-4e1d-85c0-900fc94af981",
        "_uuid": "4ccd6cd4-4d7d-43aa-87ef-ee8d9587adc0",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:33:02.48098Z",
          "iopub.status.busy": "2025-01-28T18:33:02.480654Z",
          "iopub.status.idle": "2025-01-28T18:33:02.581039Z",
          "shell.execute_reply": "2025-01-28T18:33:02.580294Z",
          "shell.execute_reply.started": "2025-01-28T18:33:02.480956Z"
        },
        "id": "CcwFuyLJ0uy7",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "# print(device,'\\n', model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d417dc78-03b6-46db-91e2-3de4a3dc1244",
        "_uuid": "cec20a30-186b-4fd5-bfba-56cf4f3ac5da",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:33:03.459851Z",
          "iopub.status.busy": "2025-01-28T18:33:03.4595Z",
          "iopub.status.idle": "2025-01-28T18:33:03.46948Z",
          "shell.execute_reply": "2025-01-28T18:33:03.468834Z",
          "shell.execute_reply.started": "2025-01-28T18:33:03.459821Z"
        },
        "id": "xCe9uUbR0uy7",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize metrics\n",
        "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "precision_metric = Precision(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "recall_metric = Recall(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "f1_metric = F1Score(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "confusion_matrix_metric = ConfusionMatrix(task=\"multiclass\", num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d040333-9094-40d1-a433-25346ab8f8cd",
        "_uuid": "14b261f2-830c-4ef4-b2e6-1165fd36230c",
        "collapsed": false,
        "id": "5ysI7q8O0uy7",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Train loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d6789b0-46df-48ea-81fe-7f28e3c2478e",
        "_uuid": "1f30ae58-abb7-4650-b3ad-37868862f37c",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:33:06.306545Z",
          "iopub.status.busy": "2025-01-28T18:33:06.30625Z",
          "iopub.status.idle": "2025-01-28T18:33:11.707874Z",
          "shell.execute_reply": "2025-01-28T18:33:11.706712Z",
          "shell.execute_reply.started": "2025-01-28T18:33:06.306522Z"
        },
        "id": "URK3bCt30uy_",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Reset metrics at start of epoch\n",
        "    accuracy_metric.reset()\n",
        "    precision_metric.reset()\n",
        "    recall_metric.reset()\n",
        "    f1_metric.reset()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = accuracy_metric.compute() * 100\n",
        "    epoch_precision = precision_metric.compute()\n",
        "    epoch_recall = recall_metric.compute()\n",
        "    epoch_f1 = f1_metric.compute()\n",
        "\n",
        "    print(\n",
        "        f\"--------\\nEpoch [{epoch + 1}/{num_epochs}]\"\n",
        "        f\"\\nTrain Loss: {epoch_loss:.4f}\"\n",
        "        f\"\\nTrain Metrics:\"\n",
        "        f\"\\n  Accuracy: {epoch_acc:.2f}%\"\n",
        "        f\"\\n  Precision: {epoch_precision:.2f}\"\n",
        "        f\"\\n  Recall: {epoch_recall:.2f}\"\n",
        "        f\"\\n  F1 Score: {epoch_f1:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f392619f-cba0-419b-9693-c6e8393fe0b9",
        "_uuid": "a9113657-a00c-4c6e-8721-1c5578357cf8",
        "collapsed": false,
        "id": "Hu1kyanq0uy_",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Val loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2e4e702e-1fd3-441f-8008-6933b238c91d",
        "_uuid": "f08a3ac9-0179-4c08-8ebb-924b923ed121",
        "execution": {
          "iopub.execute_input": "2025-01-28T19:09:51.392103Z",
          "iopub.status.busy": "2025-01-28T19:09:51.391705Z",
          "iopub.status.idle": "2025-01-28T19:09:51.834548Z",
          "shell.execute_reply": "2025-01-28T19:09:51.833285Z",
          "shell.execute_reply.started": "2025-01-28T19:09:51.392074Z"
        },
        "id": "atg1RlLU0uy_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "d2247bd9-84d5-4f6a-968c-8587c123b285",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "val_loss = 0.0\n",
        "\n",
        "# Reset all metrics at the start of validation\n",
        "accuracy_metric.reset()\n",
        "precision_metric.reset()\n",
        "recall_metric.reset()\n",
        "f1_metric.reset()\n",
        "confusion_matrix_metric.reset()\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "        confusion_matrix_metric.update(outputs, labels)\n",
        "\n",
        "# Compute final metrics\n",
        "val_loss = val_loss / len(val_loader)\n",
        "val_accuracy = accuracy_metric.compute() * 100\n",
        "val_precision = precision_metric.compute()\n",
        "val_recall = recall_metric.compute()\n",
        "val_f1 = f1_metric.compute()\n",
        "val_confusion_matrix = confusion_matrix_metric.compute()\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
        "print(f\"Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{val_confusion_matrix.cpu().numpy()}\")\n",
        "\n",
        "# Confusion matrix\n",
        "# [[TN  FP]\n",
        "#  [FN  TP]]\n",
        "\n",
        "# TN = True Negatives\n",
        "# FP = False Positives\n",
        "# FN = False Negatives\n",
        "# TP = True Positives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "604b75fd-b95e-44d8-85bd-57aaada85262",
        "_uuid": "d5417bdc-e26c-466c-b9b8-d9dd7852e6b1",
        "collapsed": false,
        "id": "QEUwJhIF0uzA",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Test loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c910cc1a-4eeb-4b67-8432-eab0ca0b28e5",
        "_uuid": "74d94bf2-cec9-4144-a784-d4f77dd7b069",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:26:21.447296Z",
          "iopub.status.busy": "2025-01-28T18:26:21.446978Z",
          "iopub.status.idle": "2025-01-28T18:26:21.506242Z",
          "shell.execute_reply": "2025-01-28T18:26:21.505458Z",
          "shell.execute_reply.started": "2025-01-28T18:26:21.447272Z"
        },
        "id": "3UJizX1N0uzA",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "\n",
        "# Reset all metrics before testing\n",
        "accuracy_metric.reset()\n",
        "precision_metric.reset()\n",
        "recall_metric.reset()\n",
        "f1_metric.reset()\n",
        "confusion_matrix_metric.reset()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "        confusion_matrix_metric.update(outputs, labels)\n",
        "\n",
        "# Compute final metrics\n",
        "test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = accuracy_metric.compute() * 100\n",
        "test_precision = precision_metric.compute()\n",
        "test_recall = recall_metric.compute()\n",
        "test_f1 = f1_metric.compute()\n",
        "test_confusion_matrix = confusion_matrix_metric.compute()\n",
        "\n",
        "print(\n",
        "    f\"Test Metrics:\"\n",
        "    f\"\\n  Loss: {test_loss:.4f}\"\n",
        "    f\"\\n  Accuracy: {test_accuracy:.2f}%\"\n",
        "    f\"\\n  Precision: {test_precision:.2f}\"\n",
        "    f\"\\n  Recall: {test_recall:.2f}\"\n",
        "    f\"\\n  F1 Score: {test_f1:.2f}\"\n",
        ")\n",
        "print(f\"Confusion Matrix:\\n{test_confusion_matrix.cpu().numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "82f19cd1-bae1-4a8a-a197-11047acc8da4",
        "_uuid": "6e451ee8-14e9-4bdf-9d96-9a006fe11ac4",
        "collapsed": false,
        "id": "UXsLwZTs0uzA",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Exporting model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_accuracy = 98.30\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "194c87f5-07c6-4d39-8a8a-a0db95fbd676",
        "_uuid": "a0d16e29-97fb-43ac-8bf2-53598da84a00",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:27:24.696383Z",
          "iopub.status.busy": "2025-01-28T18:27:24.696081Z",
          "iopub.status.idle": "2025-01-28T18:27:25.150888Z",
          "shell.execute_reply": "2025-01-28T18:27:25.149806Z",
          "shell.execute_reply.started": "2025-01-28T18:27:24.696358Z"
        },
        "id": "mJkgzC7a0uzA",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), '/kaggle/working/model/acc99.49_test-1_deepfake_detector_resnext50.pth')\n",
        "\n",
        "model_path = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/DMI_backend/ML_Models/V3_AI_image_detector_resnext101_32x8d_acc{val_accuracy:.2f}_epochs{num_epochs}.pth\"\n",
        "# torch.save(model, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Model to Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project root directory to Python path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# import the helper\n",
        "from Hugging_face_helper.helper.main import HuggingFaceHelper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "hf_helper = HuggingFaceHelper(\n",
        "    token=hf_token,\n",
        "    repo_name=\"spectrewolf8/DMI_FYP_Models_Repo\",\n",
        "    repo_local_dir=\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/Hugging_face_helper/repo/\",\n",
        "    cache_dir=\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/Hugging_face_helper/cache/\",\n",
        ")\n",
        "\n",
        "local_model_path = model_path\n",
        "filename = model_path.split(\"/\")[-1]\n",
        "\n",
        "# Upload the model\n",
        "hf_helper.upload_model(local_model_path)\n",
        "\n",
        "# Download the model back\n",
        "downloaded_model = hf_helper.download_model(filename)\n",
        "print(\"Model downloaded to:\", downloaded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "38594423-d5ee-4342-92f9-d004ff3224c0",
        "_uuid": "b06fe734-e5a1-4ba6-97c6-bccf4f4d55df",
        "collapsed": false,
        "id": "WVBFyUe50uzB",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Manual Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f1a59a4-78e6-479d-9173-b7fe5f9b896f",
        "_uuid": "0ee26b8d-916e-4ede-b228-ca79fa8f73d0",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:27:29.65262Z",
          "iopub.status.busy": "2025-01-28T18:27:29.652296Z",
          "iopub.status.idle": "2025-01-28T18:27:29.658026Z",
          "shell.execute_reply": "2025-01-28T18:27:29.657246Z",
          "shell.execute_reply.started": "2025-01-28T18:27:29.652593Z"
        },
        "id": "FcGhrNDh0uzB",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_test_image(image_path, show_image=False):\n",
        "    if show_image:\n",
        "        cv_img = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "        cv_img_rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the image with detected faces\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(cv_img_rgb)\n",
        "        plt.axis(\"off\")  # Hide axis\n",
        "        plt.title(f\"Test image\")\n",
        "        plt.show()\n",
        "\n",
        "    #     Define the transformations (should be the same as used in training)\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((224, 224)),  # Resize the image to the input size of the model\n",
        "            transforms.ToTensor(),  # Convert image to tensor\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "            ),  # Normalize as per the pre-trained model's requirements\n",
        "        ]\n",
        "    )*100:.2f\n",
        "\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply the transformations\n",
        "    image = transform(image)\n",
        "\n",
        "    # Add a batch dimension (models expect a batch of images, even if it's just one image)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "398f1864-6bef-49bc-977b-a6bd535710ca",
        "_uuid": "5f106557-31bd-4993-90de-d5be5604b0bc",
        "execution": {
          "iopub.execute_input": "2025-01-28T18:29:04.400743Z",
          "iopub.status.busy": "2025-01-28T18:29:04.40041Z",
          "iopub.status.idle": "2025-01-28T18:29:04.742604Z",
          "shell.execute_reply": "2025-01-28T18:29:04.741921Z",
          "shell.execute_reply.started": "2025-01-28T18:29:04.400693Z"
        },
        "id": "ZpC4koUg0uzC",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# image_path = f\"{DATASET_DIR}/real/000000_103.jpg\"\n",
        "image_path = f\"/home/b450-plus/Downloads/Outdoors-man-portrait_(cropped).jpg\"\n",
        "image = load_test_image(image_path=image_path, show_image=True)\n",
        "# Make prediction\n",
        "with torch.no_grad():  # No need to compute gradients for inference\n",
        "    image = image.to(device)\n",
        "    output = model(image)  # Forward pass\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = tnf.softmax(output, dim=1)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    confidence, predicted = torch.max(probabilities, 1)  # Get the class with highest probability\n",
        "\n",
        "# Convert the prediction to a label (assuming you have a mapping of class indices to labels)\n",
        "label_map = {0: \"real\", 1: \"fake\"}  # Adjust based on your dataset\n",
        "predicted_label = label_map[predicted.item()]\n",
        "\n",
        "# Print predicted label and confidence score\n",
        "confidence_score = confidence.item()\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "print(f\"Confidence score: {confidence_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a378654c-159a-41e0-b32e-e70040b2eeda",
        "_uuid": "b0823435-9642-492d-8c6e-0f8778b243c8",
        "collapsed": false,
        "id": "ff5gBB1S0uzC",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "# Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83af0487-3b54-4c09-ac32-b7ab5fac9f18",
        "_uuid": "9de33d47-fe31-4c7d-b6d1-536da4f0564b",
        "id": "qwI8vypI0uzC",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Assuming your model is already defined and loaded\n",
        "# model = models.resnext50_32x4d()  # Replace with your actual model definition\n",
        "loaded_model = torch.load(\n",
        "    \"/kaggle/working/model/acc98.00_test-2.1_CROPS_deepfake_detector_resnext101_32x8d.pth\",\n",
        "    map_location=\"cpu\",\n",
        "    weights_only=False,\n",
        ")  # Load the trained model weights\n",
        "loaded_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# If using GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aildTig0uzC"
      },
      "source": [
        "# Shifting to pytorch lightening\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-28T18:47:30.802801Z",
          "iopub.status.busy": "2025-01-28T18:47:30.802401Z",
          "iopub.status.idle": "2025-01-28T18:47:31.779834Z",
          "shell.execute_reply": "2025-01-28T18:47:31.778937Z",
          "shell.execute_reply.started": "2025-01-28T18:47:30.80277Z"
        },
        "id": "Cd8zJ8dZ0uzC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import pytorch_lightning as pl\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
        "# from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# class AIImageDetector(pl.LightningModule):\n",
        "#     def __init__(self, learning_rate=0.001):\n",
        "#         super().__init__()\n",
        "#         # Load the ResNeXt model\n",
        "#         self.model = models.resnext101_64x4d(weights='DEFAULT')\n",
        "\n",
        "#         # Modify the last layer for binary classification\n",
        "#         num_ftrs = self.model.fc.in_features\n",
        "#         self.model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#         # Save hyperparameters\n",
        "#         self.save_hyperparameters()\n",
        "\n",
        "#         # Loss function\n",
        "#         self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#         # Metrics\n",
        "#         self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "#         self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "#         self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "\n",
        "#         self.train_precision = Precision(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.val_precision = Precision(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.test_precision = Precision(task=\"multiclass\", num_classes=2, average='macro')\n",
        "\n",
        "#         self.train_recall = Recall(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.val_recall = Recall(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.test_recall = Recall(task=\"multiclass\", num_classes=2, average='macro')\n",
        "\n",
        "#         self.train_f1 = F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.val_f1 = F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
        "#         self.test_f1 = F1Score(task=\"multiclass\", num_classes=2, average='macro')\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         x, y = batch\n",
        "#         logits = self(x)\n",
        "#         loss = self.criterion(logits, y)\n",
        "\n",
        "#         # Calculate and log metrics\n",
        "#         self.train_accuracy(logits, y)\n",
        "#         self.train_precision(logits, y)\n",
        "#         self.train_recall(logits, y)\n",
        "#         self.train_f1(logits, y)\n",
        "\n",
        "#         self.log('train_loss', loss, prog_bar=True)\n",
        "#         self.log('train_accuracy', self.train_accuracy, prog_bar=True)\n",
        "#         self.log('train_precision', self.train_precision)\n",
        "#         self.log('train_recall', self.train_recall)\n",
        "#         self.log('train_f1', self.train_f1)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         x, y = batch\n",
        "#         logits = self(x)\n",
        "#         loss = self.criterion(logits, y)\n",
        "\n",
        "#         # Calculate and log metrics\n",
        "#         self.val_accuracy(logits, y)\n",
        "#         self.val_precision(logits, y)\n",
        "#         self.val_recall(logits, y)\n",
        "#         self.val_f1(logits, y)\n",
        "\n",
        "#         self.log('val_loss', loss, prog_bar=True)\n",
        "#         self.log('val_accuracy', self.val_accuracy, prog_bar=True)\n",
        "#         self.log('val_precision', self.val_precision)\n",
        "#         self.log('val_recall', self.val_recall)\n",
        "#         self.log('val_f1', self.val_f1)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def test_step(self, batch, batch_idx):\n",
        "#         x, y = batch\n",
        "#         logits = self(x)\n",
        "#         loss = self.criterion(logits, y)\n",
        "\n",
        "#         # Calculate and log metrics\n",
        "#         self.test_accuracy(logits, y)\n",
        "#         self.test_precision(logits, y)\n",
        "#         self.test_recall(logits, y)\n",
        "#         self.test_f1(logits, y)\n",
        "\n",
        "#         self.log('test_loss', loss, prog_bar=True)\n",
        "#         self.log('test_accuracy', self.test_accuracy, prog_bar=True)\n",
        "#         self.log('test_precision', self.test_precision)\n",
        "#         self.log('test_recall', self.test_recall)\n",
        "#         self.log('test_f1', self.test_f1)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "#         return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-28T18:47:34.744787Z",
          "iopub.status.busy": "2025-01-28T18:47:34.744406Z",
          "iopub.status.idle": "2025-01-28T18:47:34.749626Z",
          "shell.execute_reply": "2025-01-28T18:47:34.748594Z",
          "shell.execute_reply.started": "2025-01-28T18:47:34.744748Z"
        },
        "id": "XK_aCUYS0uzD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Create PyTorch Lightning DataModule\n",
        "# class AIImageDataModule(pl.LightningDataModule):\n",
        "#     def __init__(self, train_loader, val_loader, test_loader):\n",
        "#         super().__init__()\n",
        "#         self.train_loader = train_loader\n",
        "#         self.val_loader = val_loader\n",
        "#         self.test_loader = test_loader\n",
        "\n",
        "#     def train_dataloader(self):\n",
        "#         return self.train_loader\n",
        "\n",
        "#     def val_dataloader(self):\n",
        "#         return self.val_loader\n",
        "\n",
        "#     def test_dataloader(self):\n",
        "#         return self.test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-28T18:47:36.777687Z",
          "iopub.status.busy": "2025-01-28T18:47:36.777348Z",
          "iopub.status.idle": "2025-01-28T18:47:36.7837Z",
          "shell.execute_reply": "2025-01-28T18:47:36.782947Z",
          "shell.execute_reply.started": "2025-01-28T18:47:36.777656Z"
        },
        "id": "ZPUmKnJb0uzD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from pytorch_lightning.loggers import WandbLogger  # or TensorBoardLogger\n",
        "\n",
        "# # Initialize logger\n",
        "# wandb_logger = WandbLogger(project='ai-image-detection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "",
            "75a8d0ae9159416da04c75457436fcb3",
            "f25828bab9854cde8281b1c23c07c176"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-01-28T18:51:37.515454Z",
          "iopub.status.busy": "2025-01-28T18:51:37.515159Z",
          "iopub.status.idle": "2025-01-28T19:08:42.385817Z",
          "shell.execute_reply": "2025-01-28T19:08:42.385048Z",
          "shell.execute_reply.started": "2025-01-28T18:51:37.515431Z"
        },
        "id": "FJxnzxgT0uzD",
        "outputId": "af32a8da-5a10-4f05-f607-5dc9caee427e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Create the data module\n",
        "# data_module = AIImageDataModule(train_loader, val_loader, test_loader)\n",
        "\n",
        "# # Create the model\n",
        "# model = AIImageDetector(learning_rate=0.001)\n",
        "\n",
        "# # Define callbacks\n",
        "# checkpoint_callback = ModelCheckpoint(\n",
        "#     monitor='val_accuracy',\n",
        "#     dirpath='checkpoints/',\n",
        "#     filename='ai-detector-{epoch:02d}-{val_accuracy:.2f}',\n",
        "#     save_top_k=3,\n",
        "#     mode='max'\n",
        "# )\n",
        "\n",
        "# early_stop_callback = EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=5,\n",
        "#     mode='min'\n",
        "# )\n",
        "\n",
        "# # Initialize trainer\n",
        "# trainer = pl.Trainer(\n",
        "#     max_epochs=15,\n",
        "#     accelerator='auto',  # Automatically detect GPU/CPU\n",
        "#     devices=1,\n",
        "#     callbacks=[checkpoint_callback, early_stop_callback],\n",
        "#     # logger=wandb_logger\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.fit(model, data_module)\n",
        "\n",
        "# # Test the model\n",
        "# trainer.test(model, data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-28T19:09:37.008861Z",
          "iopub.status.busy": "2025-01-28T19:09:37.008532Z",
          "iopub.status.idle": "2025-01-28T19:09:37.403988Z",
          "shell.execute_reply": "2025-01-28T19:09:37.403227Z",
          "shell.execute_reply.started": "2025-01-28T19:09:37.008837Z"
        },
        "id": "aM5iwATq0uzD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# torch.save(model, '/kaggle/working/models/acc94.00_test-2.0_AI_image_detector_resnext101_32x8d.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2biGuE2a0uzE",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "AIGM_detectection-ResNext-FYP-P2.ipynb",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2811380,
          "sourceId": 7501727,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6501494,
          "sourceId": 10500820,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "DMI_FYP_dj_primary-backend-cvNQsyiC",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
