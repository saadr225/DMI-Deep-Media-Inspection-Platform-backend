{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "298c1ee3-6c05-4d42-87d2-9def99720b3b",
        "_uuid": "8f1a6179-2ff2-4b45-bd26-efcabea5b9ba",
        "collapsed": false,
        "id": "FWgCVkTpenkT",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "# Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7de1e3b1-eaf3-4c8f-9596-13232714db87",
        "_uuid": "b192ef50-a1eb-4019-911d-cab92f2e5a6d",
        "id": "8hysbP3renkU",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install opencv-python --upgrade\n",
        "# !pip install ultralytics --upgrade\n",
        "# !pip install torchmetrics --upgrade\n",
        "# !pip install grad-cam --upgrade\n",
        "# !pip install natsort --upgrade\n",
        "# !pip install Pillow --upgrade\n",
        "# !pip install wandb --upgrade\n",
        "# # !pip install deepface --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "247199a1-67da-47d0-a48c-3dad038d7531",
        "_uuid": "34fcdb79-2c61-4162-a5c4-1e535a82b391",
        "collapsed": false,
        "id": "My6zTzPtenkU",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "### >Restart kernel!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "d9c8b144-82db-4987-9cab-bcd34311fc8e",
        "_uuid": "dcfc7208-568a-41ac-a227-0f5fa74f9fc7",
        "execution": {
          "iopub.execute_input": "2025-01-24T18:15:13.511522Z",
          "iopub.status.busy": "2025-01-24T18:15:13.511118Z",
          "iopub.status.idle": "2025-01-24T18:15:22.706696Z",
          "shell.execute_reply": "2025-01-24T18:15:22.705793Z",
          "shell.execute_reply.started": "2025-01-24T18:15:13.511487Z"
        },
        "id": "-LXgu1CUenkV",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "98324e6b-aeea-40eb-f1b0-92ffc99528ea",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as tnf\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image, ImageChops\n",
        "import os, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n",
        "from ultralytics import YOLO\n",
        "import hashlib\n",
        "from natsort import natsorted\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "65bbb7a6-22ab-4386-89a3-cf23164cf4df",
        "_uuid": "5849a0c2-018c-4338-9594-acc8b8143648",
        "collapsed": false,
        "id": "4BFGZN92enkW",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3d9055d0-782c-4db6-a6ed-566214240536",
        "_uuid": "545cea49-4110-4e58-bc42-72f6731bd065",
        "collapsed": false,
        "id": "PMnvxTyzenkW",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Generate Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6948f533-75bf-4b7f-b7ca-aba8e1daed40",
        "_uuid": "2dd2f000-dccd-4ff0-a3f9-cf67449f18f0",
        "collapsed": false,
        "id": "_tcqruzNenkX",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "### Testing different approaches in facial detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d45152e1-082c-4d28-b2cd-14b54dbe3cbd",
        "_uuid": "c4813236-51ec-4e58-b9ad-83f1f76902c0",
        "id": "kSH6EU7Kenkb",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Directory containing frames\n",
        "frame_dir = \"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline\"\n",
        "\n",
        "# List of image files in the directory\n",
        "frame_files = natsorted(\n",
        "    [f for f in os.listdir(frame_dir) if os.path.isfile(os.path.join(frame_dir, f))]\n",
        ")\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")  # Replace with the appropriate model if needed\n",
        "\n",
        "# Process frames from the directory\n",
        "for frame_file in frame_files:\n",
        "    # Load the frame (image) using OpenCV\n",
        "    frame_path = os.path.join(frame_dir, frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    # Run detection\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract the boxes, scores, and classes from results\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Get bounding box coordinates\n",
        "    scores = results[0].boxes.conf.cpu().numpy()  # Get confidence scores\n",
        "    classes = results[0].boxes.cls.cpu().numpy()  # Get class IDs\n",
        "\n",
        "    print(boxes, classes)\n",
        "    # Filter and draw boxes for persons (class ID 0)\n",
        "    for box, score, cls in zip(boxes, scores, classes):\n",
        "        if cls == 0 and score >= 0.3:  # Class ID 0 is typically for persons, adjust threshold as needed\n",
        "            x1, y1, x2, y2 = map(int, box)  # Convert box to integer\n",
        "            label = f\"Person: {score:.2f}\"\n",
        "            # Draw rectangle around persons\n",
        "            h_padding = 0 / 100  # Horizontal padding in percent\n",
        "            v_padding = 0 / 100  # Vertical padding in percent\n",
        "            include_head_offset = 0  # 2.5  # Offset to include head in the resultant crops\n",
        "            cv2.rectangle(\n",
        "                frame,\n",
        "                (x1 - int(x1 * h_padding), y1 - int(y1 * include_head_offset * v_padding)),\n",
        "                (x2 + int(x2 * h_padding), y2 + int(y2 * v_padding)),\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    # Convert back to RGB for Matplotlib\n",
        "    rgb_frame_with_faces = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the frame with person boxes using Matplotlib\n",
        "    plt.imshow(rgb_frame_with_faces)\n",
        "    plt.title(f\"Frame: {frame_file}\")\n",
        "    plt.axis(\"off\")  # Hide axes\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4993e655-0624-4f33-962d-a6a8ab9a4b8f",
        "_uuid": "1b62e947-ab5f-4137-9214-ee0ec3d110a9",
        "collapsed": false,
        "id": "NN6nNeuEenkb",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "### Face detection pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "33d46889-8026-456a-a507-150d170153e8",
        "_uuid": "3aa03989-6a22-4c0f-ab1d-e22264adc426",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:12:53.068668Z",
          "iopub.status.busy": "2024-11-17T18:12:53.068125Z",
          "iopub.status.idle": "2024-11-17T18:12:53.079497Z",
          "shell.execute_reply": "2024-11-17T18:12:53.078011Z",
          "shell.execute_reply.started": "2024-11-17T18:12:53.068622Z"
        },
        "id": "7XgrkP1Benkb",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Detect face using haar cascade\n",
        "def detect_face_depracted(image_path, log_level=0):\n",
        "    try:\n",
        "        # Load the Haar cascade file\n",
        "        face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "        )\n",
        "\n",
        "        # Read the image\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        if img is None:\n",
        "            if log_level >= 2:\n",
        "                print(f\"Unable to read {image_path}. Skipping...\")\n",
        "            return False\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            if log_level >= 2:\n",
        "                print(f\"Face detected in {image_path}\")\n",
        "            return faces, True  # Return detected faces\n",
        "        else:\n",
        "            if log_level >= 2:\n",
        "                print(f\"No face detected in {image_path}\")\n",
        "            return None, True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b41bd60-564a-45c9-9acb-d003d8506729",
        "_uuid": "f537326d-5b44-49aa-856e-953aeb7cb779",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:12:53.702646Z",
          "iopub.status.busy": "2024-11-17T18:12:53.702166Z",
          "iopub.status.idle": "2024-11-17T18:12:53.714985Z",
          "shell.execute_reply": "2024-11-17T18:12:53.713642Z",
          "shell.execute_reply.started": "2024-11-17T18:12:53.702601Z"
        },
        "id": "iyIkMSISenkb",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configuration parameters\n",
        "# video_path = '/kaggle/input/celeb-df-v2-dataset/Celeb-real/id0_0000.mp4'\n",
        "# output_dir = '/kaggle/working/temp'\n",
        "# frame_rate = 2  # Frames per second to extract\n",
        "\n",
        "\n",
        "def extract_frames_with_face_derpracated(\n",
        "    video_path, frame_output_dir, id=\"frame\", frame_rate=2, log_level=0\n",
        "):\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(frame_output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize video capture\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_interval = int(fps / frame_rate)\n",
        "\n",
        "    frame_count = 0\n",
        "    saved_frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Extract frames at the specified interval\n",
        "        if frame_count % frame_interval == 0:\n",
        "            # Temporarily save the current frame to disk\n",
        "            temp_frame_path = os.path.join(frame_output_dir, f\"{id}_{frame_count}.jpg\")\n",
        "            cv2.imwrite(temp_frame_path, frame)\n",
        "\n",
        "            try:\n",
        "                # Detect face in the frame\n",
        "                _, face_found = detect_face(image_path=temp_frame_path, log_level=log_level)\n",
        "\n",
        "                if face_found:\n",
        "                    if log_level >= 2:\n",
        "                        print(f\"Face detected in frame {frame_count}\")\n",
        "\n",
        "                    # Save the frame with a detected face\n",
        "                    output_frame_path = os.path.join(frame_output_dir, f\"{id}_{saved_frame_count}.jpg\")\n",
        "                    cv2.imwrite(output_frame_path, frame)\n",
        "                    saved_frame_count += 1\n",
        "                else:\n",
        "                    if log_level >= 2:\n",
        "                        print(f\"No face detected in frame {frame_count}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {frame_count}: {e}\")\n",
        "\n",
        "            # Remove the temporary frame file to save disk space\n",
        "            os.remove(temp_frame_path)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    # cv2.destroyAllWindows()\n",
        "    if log_level == 1:\n",
        "        # print(f\"Total frames processed: {frame_count} in {video_path}\")\n",
        "        print(f\"Frames with detected faces saved: {saved_frame_count}/{frame_count} from {video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-24T12:26:28.28617Z",
          "iopub.status.busy": "2025-01-24T12:26:28.285365Z",
          "iopub.status.idle": "2025-01-24T12:26:29.297247Z",
          "shell.execute_reply": "2025-01-24T12:26:29.296068Z",
          "shell.execute_reply.started": "2025-01-24T12:26:28.286135Z"
        },
        "id": "nVY67_Qgenkc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !mkdir '/kaggle/working/temp_dataset/fake'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25dabb54-85e8-4718-859a-4421aa60ac41",
        "_uuid": "c8f16c47-b1c8-4534-8e04-92767fb0ec3a",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:13:03.669354Z",
          "iopub.status.busy": "2024-11-17T18:13:03.668825Z",
          "iopub.status.idle": "2024-11-17T18:13:16.418812Z",
          "shell.execute_reply": "2024-11-17T18:13:16.417089Z",
          "shell.execute_reply.started": "2024-11-17T18:13:03.66931Z"
        },
        "id": "9N644mHFenkc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Directory containing frames\n",
        "frame_dir = \"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline\"\n",
        "\n",
        "# List of image files in the directory\n",
        "frame_files = natsorted(\n",
        "    [f for f in os.listdir(frame_dir) if os.path.isfile(os.path.join(frame_dir, f))]\n",
        ")\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")  # Replace with the appropriate model if needed\n",
        "\n",
        "# Process frames from the directory\n",
        "for frame_file in frame_files:\n",
        "    # Load the frame (image) using OpenCV\n",
        "    frame_path = os.path.join(frame_dir, frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    # Run detection\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract the boxes, scores, and classes from results\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Get bounding box coordinates\n",
        "    scores = results[0].boxes.conf.cpu().numpy()  # Get confidence scores\n",
        "    classes = results[0].boxes.cls.cpu().numpy()  # Get class IDs\n",
        "\n",
        "    print(boxes, classes)\n",
        "    # Filter and draw boxes for persons (class ID 0)\n",
        "    for box, score, cls in zip(boxes, scores, classes):\n",
        "        if cls == 0 and score >= 0.3:  # Class ID 0 is typically for persons, adjust threshold as needed\n",
        "            x1, y1, x2, y2 = map(int, box)  # Convert box to integer\n",
        "            label = f\"Person: {score:.2f}\"\n",
        "            # Draw rectangle around persons\n",
        "            h_padding = 0 / 100  # Horizontal padding in percent\n",
        "            v_padding = 0 / 100  # Vertical padding in percent\n",
        "            include_head_offset = 0  # 2.5  # Offset to include head in the resultant crops\n",
        "            cv2.rectangle(\n",
        "                frame,\n",
        "                (x1 - int(x1 * h_padding), y1 - int(y1 * include_head_offset * v_padding)),\n",
        "                (x2 + int(x2 * h_padding), y2 + int(y2 * v_padding)),\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    # Convert back to RGB for Matplotlib\n",
        "    rgb_frame_with_faces = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the frame with person boxes using Matplotlib\n",
        "    plt.imshow(rgb_frame_with_faces)\n",
        "    plt.title(f\"Frame: {frame_file}\")\n",
        "    plt.axis(\"off\")  # Hide axes\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71675e40-66a6-405e-90fb-65211291011d",
        "_uuid": "f82e0424-0aa7-4eaf-b0a4-a41091561b24",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:13:54.366179Z",
          "iopub.status.busy": "2024-11-17T18:13:54.36575Z",
          "iopub.status.idle": "2024-11-17T18:13:54.442005Z",
          "shell.execute_reply": "2024-11-17T18:13:54.440762Z",
          "shell.execute_reply.started": "2024-11-17T18:13:54.366138Z"
        },
        "id": "ix5GY2gSenkc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")  # Replace with the appropriate model if needed\n",
        "\n",
        "\n",
        "# Detect face using YOLOv8\n",
        "def detect_face(frame, threshold=0.5, log_level=0):\n",
        "    try:\n",
        "        # Run detection\n",
        "        results = model(frame)\n",
        "\n",
        "        # Extract the boxes, scores, and classes from results\n",
        "        boxes = results[0].boxes.xyxy.cpu().numpy()  # Get bounding box coordinates\n",
        "        scores = results[0].boxes.conf.cpu().numpy()  # Get confidence scores\n",
        "        classes = results[0].boxes.cls.cpu().numpy()  # Get class IDs\n",
        "\n",
        "        detected_faces = {}\n",
        "        index = 0\n",
        "        face_detected = False\n",
        "\n",
        "        # Filter and save boxes for persons (class ID 0 typically corresponds to persons/faces)\n",
        "        for box, score, object_class in zip(boxes, scores, classes):\n",
        "            if (\n",
        "                object_class == 0 and score > threshold\n",
        "            ):  # Class ID 0 is typically for persons, adjust as needed\n",
        "                x1, y1, x2, y2 = map(int, box)  # Convert box coordinates to integer\n",
        "                top_left = (x1, y1)\n",
        "                bottom_right = (x2, y2)\n",
        "\n",
        "                # Save the detected face box coordinates\n",
        "                detected_faces[index] = {\n",
        "                    \"top_left\": top_left,\n",
        "                    \"bottom_right\": bottom_right,\n",
        "                    \"score\": score,\n",
        "                }\n",
        "                index += 1\n",
        "                face_detected = True  # Set flag to True as faces were detected\n",
        "\n",
        "        # Logging if required\n",
        "        if log_level >= 2:\n",
        "            if face_detected:\n",
        "                print(f\"Faces detected in {image_path}\")\n",
        "            else:\n",
        "                print(f\"No faces detected in {image_path}\")\n",
        "\n",
        "        return detected_faces, face_detected  # Return the faces and detection status\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None, False  # Return None and False in case of an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3172d612-9b1a-41a2-a5e8-8ea450413ba6",
        "_uuid": "1a4bc8f8-d094-4ce2-bc06-695dada332de",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:14:00.698169Z",
          "iopub.status.busy": "2024-11-17T18:14:00.697745Z",
          "iopub.status.idle": "2024-11-17T18:14:00.934443Z",
          "shell.execute_reply": "2024-11-17T18:14:00.933111Z",
          "shell.execute_reply.started": "2024-11-17T18:14:00.698128Z"
        },
        "id": "WB0c25xJenkc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_path = \"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/gettyimages-1425797951-612x612.jpg\"\n",
        "# Check if the image path is valid\n",
        "if not os.path.isfile(image_path):\n",
        "    raise FileNotFoundError(f\"Image file {image_path} not found\")\n",
        "\n",
        "# Load the frame (image) using OpenCV\n",
        "frame = cv2.imread(image_path)\n",
        "print(detect_face(frame))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a46534bf-5632-47f5-81de-3315d2ed0089",
        "_uuid": "f8d310fa-8e49-4a78-8630-b86db9ba8880",
        "execution": {
          "iopub.execute_input": "2024-11-17T18:14:07.066616Z",
          "iopub.status.busy": "2024-11-17T18:14:07.066135Z",
          "iopub.status.idle": "2024-11-17T18:14:07.078016Z",
          "shell.execute_reply": "2024-11-17T18:14:07.076461Z",
          "shell.execute_reply.started": "2024-11-17T18:14:07.066571Z"
        },
        "id": "bjMo5qpNenkc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_crops(frame, frame_output_dir, frame_index, detected_faces, frame_id, show_crops=False):\n",
        "    # Save each detected face as a cropped image\n",
        "    for face_idx, face_data in detected_faces.items():\n",
        "        print(\"face_data\", face_data)\n",
        "        top_left = face_data[\"top_left\"]\n",
        "        bottom_right = face_data[\"bottom_right\"]\n",
        "\n",
        "        # Crop the face from the frame\n",
        "        face_crop = frame[top_left[1] : bottom_right[1], top_left[0] : bottom_right[0]]\n",
        "\n",
        "        # Generate output path for the face crop\n",
        "        output_face_path = os.path.join(\n",
        "            frame_output_dir, f\"{frame_id}_{frame_index}_face{face_idx}.jpg\"\n",
        "        )\n",
        "\n",
        "        # Save the face crop\n",
        "        cv2.imwrite(output_face_path, face_crop)\n",
        "\n",
        "        # Extra logic to show crops and edge coordinates for debugging\n",
        "        if show_crops:\n",
        "            # Draw red circles on the corners for debugging\n",
        "            #             cv2.circle(face_crop, (0, 0), 5, (255, 0, 0), -1)  # Top-left (0,0 in the cropped frame)\n",
        "            #             cv2.circle(face_crop, (face_crop.shape[1]-1, face_crop.shape[0]-1), 5, (255, 0, 0), -1)  # Bottom-right\n",
        "\n",
        "            # Annotate the coordinates for debugging\n",
        "            #             cv2.putText(face_crop, f\"({top_left[0]},{top_left[1]})\", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "            #             cv2.putText(face_crop, f\"({bottom_right[0]},{bottom_right[1]})\",\n",
        "            #                         (face_crop.shape[1] - 100, face_crop.shape[0] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "            # Resize the face crop to 224x224\n",
        "            face_crop = cv2.resize(face_crop, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "            # Display the face crop using matplotlib\n",
        "            plt.imshow(\n",
        "                cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
        "            )  # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
        "            plt.title(f\"Face {face_idx} from Frame {frame_index}\")\n",
        "            plt.axis(\"off\")  # Hide axes for cleaner visualization\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "940f202d-88d3-4fbf-acf8-a7f4b94236a0",
        "_uuid": "2bc15f67-bca8-40b4-887f-689dbc65578b",
        "execution": {
          "iopub.execute_input": "2024-12-13T07:26:29.637203Z",
          "iopub.status.busy": "2024-12-13T07:26:29.63674Z",
          "iopub.status.idle": "2024-12-13T07:26:30.183101Z",
          "shell.execute_reply": "2024-12-13T07:26:30.181886Z",
          "shell.execute_reply.started": "2024-12-13T07:26:29.637169Z"
        },
        "id": "hEcCFHETenkd",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ab743259-b030-4070-d66a-a379cb88276b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# List of supported image formats\n",
        "SUPPORTED_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\"]\n",
        "\n",
        "# List of supported video formats\n",
        "SUPPORTED_VIDEO_FORMATS = [\".mp4\", \".avi\", \".mov\"]\n",
        "\n",
        "\n",
        "def check_media_type(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        return \"File does not exist\"\n",
        "\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension in SUPPORTED_IMAGE_FORMATS:\n",
        "        try:\n",
        "            with Image.open(file_path) as img:\n",
        "                img.verify()\n",
        "            return \"Image\"\n",
        "        except:\n",
        "            return \"Invalid image file\"\n",
        "\n",
        "    elif file_extension in SUPPORTED_VIDEO_FORMATS:\n",
        "        try:\n",
        "            video = cv2.VideoCapture(file_path)\n",
        "            if video.isOpened():\n",
        "                video.release()\n",
        "                return \"Video\"\n",
        "            else:\n",
        "                return \"Invalid video file\"\n",
        "        except:\n",
        "            return \"Invalid video file\"\n",
        "\n",
        "    else:\n",
        "        return \"Not a supported image or video format\"\n",
        "\n",
        "\n",
        "def process_media_file(media_path, output_dir, frame_id=\"frame\", frame_rate=2, log_level=0):\n",
        "    \"\"\"\n",
        "    Process media and extract face crops\n",
        "    \"\"\"\n",
        "    media_type = check_media_type(media_path)\n",
        "\n",
        "    if media_type == \"Image\":\n",
        "        process_image(media_path, output_dir, frame_id, log_level)\n",
        "    elif media_type == \"Video\":\n",
        "        process_video(media_path, output_dir, frame_id, frame_rate, log_level)\n",
        "    else:\n",
        "        print(f\"Unsupported media type: {media_type}\")\n",
        "\n",
        "\n",
        "def process_image(image_path, output_dir, frame_id=\"frame\", log_level=0):\n",
        "    try:\n",
        "        frame = cv2.imread(image_path)\n",
        "        detected_faces, face_found = detect_face(frame=frame, threshold=0.3, log_level=log_level)\n",
        "        if face_found:\n",
        "            if log_level >= 2:\n",
        "                print(f\"Face(s) detected in image {image_path}\")\n",
        "            generate_crops(\n",
        "                frame=frame,\n",
        "                frame_output_dir=output_dir,\n",
        "                frame_index=0,\n",
        "                detected_faces=detected_faces,\n",
        "                frame_id=frame_id,\n",
        "                show_crops=False,\n",
        "            )\n",
        "        else:\n",
        "            if log_level >= 2:\n",
        "                print(f\"No face detected in image {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "\n",
        "\n",
        "def process_video(video_path, output_dir, frame_id, frame_rate, log_level=0):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_interval = int(fps / frame_rate)\n",
        "    frame_count = 0\n",
        "    saved_frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % frame_interval == 0:\n",
        "            try:\n",
        "                detected_faces, face_found = detect_face(\n",
        "                    frame=frame, threshold=0.4, log_level=log_level\n",
        "                )\n",
        "                if face_found:\n",
        "                    if log_level >= 2:\n",
        "                        print(f\"Face(s) detected in frame {frame_count}\")\n",
        "                    generate_crops(\n",
        "                        frame=frame,\n",
        "                        frame_output_dir=output_dir,\n",
        "                        frame_index=saved_frame_count,\n",
        "                        detected_faces=detected_faces,\n",
        "                        frame_id=frame_id,\n",
        "                        show_crops=False,\n",
        "                    )\n",
        "                    saved_frame_count += 1\n",
        "                else:\n",
        "                    if log_level >= 2:\n",
        "                        print(f\"No face detected in frame {frame_count}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {frame_count}: {e}\")\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if log_level == 1:\n",
        "        print(f\"Frames with detected faces saved: {saved_frame_count}/{frame_count} from {video_path}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "media_path = f\"{DATA_SET_DIR}/Celeb-synthesis/id0_id26_0008.mp4\"\n",
        "output_dir = \"/kaggle/working/temp_face_crops\"\n",
        "process_media_file(media_path, output_dir, frame_id=\"frame\", frame_rate=2, log_level=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c33fbf53-d5ba-4a94-8287-854e60441f5a",
        "_uuid": "c71b8a22-f780-4050-85c9-3443efef5798",
        "execution": {
          "iopub.execute_input": "2025-01-24T12:24:48.345823Z",
          "iopub.status.busy": "2025-01-24T12:24:48.345382Z",
          "iopub.status.idle": "2025-01-24T12:24:48.371725Z",
          "shell.execute_reply": "2025-01-24T12:24:48.370829Z",
          "shell.execute_reply.started": "2025-01-24T12:24:48.345764Z"
        },
        "id": "PAfd3PATenkd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MediaProcessor:\n",
        "    \"\"\"\n",
        "    A class for detecting and extracting faces from images and videos using YOLOv8.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path=\"yolov8n.pt\", threshold=0.5, log_level=0, FRAMES_FILE_FORMAT=\"jpg\"):\n",
        "        \"\"\"\n",
        "        Initialize the FaceDetector.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the YOLOv8 model file.\n",
        "            threshold (float): Confidence threshold for face detection.\n",
        "            log_level (int): Level of logging (0: None, 1: Basic, 2: Verbose).\n",
        "        \"\"\"\n",
        "        self.model = YOLO(model_path)\n",
        "        self.threshold = threshold\n",
        "        self.log_level = log_level\n",
        "        self.supported_image_formats = [\".jpg\", \".jpeg\", \".png\"]\n",
        "        self.supported_video_formats = [\".mp4\", \".avi\", \".mov\"]\n",
        "        self.FRAMES_FILE_FORMAT = FRAMES_FILE_FORMAT\n",
        "        print(\"Warning: frame_rate value ignored for Image media\\nReason: Image input\")\n",
        "        print(\n",
        "            \"\\nWarning: naming scheme: \\nImage: {file_content_hash}_{file_name_hash}_{frame_index=0}_{crop_index}.{extension}\\nVideo: {file_content_hash}_{file_name_hash}_{frame_index}_{crop_index}.{extension}\\n\"\n",
        "        )\n",
        "\n",
        "    def detect_face(self, frame):\n",
        "        \"\"\"\n",
        "        Detect faces in a given frame.\n",
        "\n",
        "        Args:\n",
        "            frame (numpy.ndarray): Input image frame.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Detected faces (dict) and a boolean indicating if faces were found.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Run detection\n",
        "            if self.log_level >= 3:\n",
        "                results = self.model(frame)\n",
        "            else:\n",
        "                results = self.model(frame, verbose=False)\n",
        "\n",
        "            # Extract boxes, scores, and classes\n",
        "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "            scores = results[0].boxes.conf.cpu().numpy()\n",
        "            classes = results[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "            detected_faces = {}\n",
        "            face_detected = False\n",
        "\n",
        "            # Filter and save boxes for persons (class ID 0)\n",
        "            for index, (box, score, object_class) in enumerate(zip(boxes, scores, classes)):\n",
        "                if object_class == 0 and score > self.threshold:\n",
        "                    x1, y1, x2, y2 = map(int, box)\n",
        "                    detected_faces[index] = {\n",
        "                        \"top_left\": (x1, y1),\n",
        "                        \"bottom_right\": (x2, y2),\n",
        "                        \"score\": score,\n",
        "                    }\n",
        "                    face_detected = True\n",
        "\n",
        "            # Logging\n",
        "            if self.log_level >= 2:\n",
        "                print(f\"{'Faces' if face_detected else 'No faces'} detected in frame\")\n",
        "\n",
        "            return detected_faces, face_detected\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame: {e}\")\n",
        "            return None, False\n",
        "\n",
        "    def generate_crops(\n",
        "        self, frame, output_dir, frame_index, detected_faces, frame_id, show_crops=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generate and save face crops from a frame.\n",
        "\n",
        "        Args:\n",
        "            frame (numpy.ndarray): Input image frame.\n",
        "            output_dir (str): Directory to save face crops.\n",
        "            frame_index (int): Index of the current frame.\n",
        "            detected_faces (dict): Dictionary of detected faces.\n",
        "            frame_id (str): Identifier for the frame.\n",
        "            show_crops (bool): Whether to display the crops (for debugging).\n",
        "        \"\"\"\n",
        "        for face_idx, face_data in detected_faces.items():\n",
        "            top_left = face_data[\"top_left\"]\n",
        "            bottom_right = face_data[\"bottom_right\"]\n",
        "\n",
        "            # Crop the face from the frame\n",
        "            face_crop = frame[top_left[1] : bottom_right[1], top_left[0] : bottom_right[0]]\n",
        "\n",
        "            if show_crops:\n",
        "                # Resize and display the face crop (for debugging)\n",
        "                face_crop_resized = cv2.resize(face_crop, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "                cv2.imshow(f\"Face {face_idx} from Frame {frame_index}\", face_crop_resized)\n",
        "                cv2.waitKey(0)\n",
        "                cv2.destroyAllWindows()\n",
        "\n",
        "            # Generate output path for the face crop\n",
        "            output_face_path = os.path.join(\n",
        "                output_dir, f\"{frame_id}_{frame_index}_{face_idx}.{self.FRAMES_FILE_FORMAT}\"\n",
        "            )\n",
        "\n",
        "            # Check if the crop already exists\n",
        "            if os.path.exists(output_face_path):\n",
        "                if self.log_level >= 2:\n",
        "                    print(f\"Skipping crop {output_face_path}: already exists\")\n",
        "                continue\n",
        "            if self.log_level >= 1:\n",
        "                print(\"Crop saved at: \", output_face_path)\n",
        "            # Save the face crop\n",
        "            cv2.imwrite(output_face_path, face_crop)\n",
        "\n",
        "    def check_media_type(self, file_path):\n",
        "        \"\"\"\n",
        "        Check the type of media file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the media file.\n",
        "\n",
        "        Returns:\n",
        "            str: Type of media ('Image', 'Video', or error message).\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            return \"File does not exist\"\n",
        "\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        if file_extension in self.supported_image_formats:\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.verify()\n",
        "                return \"Image\"\n",
        "            except:\n",
        "                return \"Invalid image file\"\n",
        "        elif file_extension in self.supported_video_formats:\n",
        "            try:\n",
        "                video = cv2.VideoCapture(file_path)\n",
        "                if video.isOpened():\n",
        "                    video.release()\n",
        "                    return \"Video\"\n",
        "                else:\n",
        "                    return \"Invalid video file\"\n",
        "            except:\n",
        "                return \"Invalid video file\"\n",
        "        else:\n",
        "            return \"Not a supported image or video format\"\n",
        "\n",
        "    def process_image(self, image_path, output_dir, frame_id, generate_crops_flag):\n",
        "        \"\"\"\n",
        "        Process a single image and extract face crops.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the input image.\n",
        "            output_dir (str): Directory to save face crops.\n",
        "            frame_id (str): Identifier for the frame.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            frame = cv2.imread(image_path)\n",
        "\n",
        "            detected_faces, face_found = self.detect_face(frame)\n",
        "\n",
        "            if face_found:\n",
        "                if self.log_level >= 2:\n",
        "                    print(f\"Face(s) detected in image {image_path}\")\n",
        "                if generate_crops_flag:\n",
        "                    self.generate_crops(frame, output_dir, 0, detected_faces, frame_id)\n",
        "                else:\n",
        "                    frame_index = 0\n",
        "                    # Generate output path for the face crop\n",
        "                    output_face_path = os.path.join(\n",
        "                        output_dir, f\"{frame_id}_{frame_index}.{self.FRAMES_FILE_FORMAT}\"\n",
        "                    )  # frame_index = 0 for all still images\n",
        "\n",
        "                    # Check if the image already exists\n",
        "                    if os.path.exists(output_face_path):\n",
        "                        if self.log_level >= 2:\n",
        "                            print(f\"Skipping image {output_face_path}: already exists\")\n",
        "                        return\n",
        "\n",
        "                    print(\"Frame saved at: \", output_face_path)\n",
        "                    # Save the frame\n",
        "                    cv2.imwrite(output_face_path, frame)\n",
        "            else:\n",
        "                if self.log_level >= 1:\n",
        "                    print(f\"No face detected in image {image_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "\n",
        "    def process_video(self, video_path, output_dir, frame_id, frame_rate, generate_crops_flag):\n",
        "        \"\"\"\n",
        "        Process a video and extract face crops from frames.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to the input video.\n",
        "            output_dir (str): Directory to save face crops.\n",
        "            frame_id (str): Identifier for the frames.\n",
        "            frame_rate (int): Rate at which to extract frames.\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_interval = int(fps / frame_rate)\n",
        "        frame_count = 0\n",
        "        saved_frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame_count % frame_interval == 0:\n",
        "                try:\n",
        "                    detected_faces, face_found = self.detect_face(frame)\n",
        "                    if face_found:\n",
        "                        if self.log_level >= 2:\n",
        "                            print(f\"Face(s) detected in frame {frame_count}\")\n",
        "                        if generate_crops_flag:\n",
        "                            self.generate_crops(\n",
        "                                frame, output_dir, saved_frame_count, detected_faces, frame_id\n",
        "                            )\n",
        "                        else:\n",
        "                            # Generate output path for the face crop\n",
        "                            output_face_path = os.path.join(\n",
        "                                output_dir, f\"{frame_id}_{saved_frame_count}.{self.FRAMES_FILE_FORMAT}\"\n",
        "                            )\n",
        "\n",
        "                            # Check if the frame already exists\n",
        "                            if os.path.exists(output_face_path):\n",
        "                                if self.log_level >= 1:\n",
        "                                    print(f\"Skipping frame {output_face_path}: already exists\")\n",
        "                            else:\n",
        "                                # Save the frame\n",
        "                                cv2.imwrite(output_face_path, frame)\n",
        "                        saved_frame_count += 1\n",
        "                    else:\n",
        "                        if self.log_level >= 2:\n",
        "                            print(f\"No face detected in frame {frame_count}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing frame {frame_count}: {e}\")\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        if self.log_level >= 1:\n",
        "            print(\n",
        "                f\"Frames with detected faces saved: {saved_frame_count}/{frame_count} from {video_path}\"\n",
        "            )\n",
        "\n",
        "    def generate_6_digit_hash(self, input_string):\n",
        "        # Create a hash object\n",
        "        hash_object = hashlib.sha256(input_string.encode())\n",
        "        # Get the hexadecimal digest of the hash\n",
        "        hex_dig = hash_object.hexdigest()\n",
        "        # Convert the first 6 characters of the hash to an integer\n",
        "        hash_int = int(hex_dig[:6], 16)\n",
        "        # Ensure the hash is 6 digits long\n",
        "        hash_6_digit = str(hash_int).zfill(6)\n",
        "        return hash_6_digit\n",
        "\n",
        "    def hash_file_content(self, file_path):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            file_content = f.read()\n",
        "        return self.generate_6_digit_hash(file_content.decode(\"latin1\"))\n",
        "\n",
        "    def hash_file_name(self, file_path):\n",
        "        file_name = os.path.basename(file_path)\n",
        "        return self.generate_6_digit_hash(file_name)\n",
        "\n",
        "    def generate_combined_hash(self, file_path):\n",
        "        file_content_hash = self.hash_file_content(file_path)\n",
        "        file_name_hash = self.hash_file_name(file_path)\n",
        "        return f\"{file_content_hash}_{file_name_hash}\"\n",
        "\n",
        "    def process_media_file(self, media_path, output_dir, generate_crops_flag, frame_rate=2):\n",
        "        \"\"\"\n",
        "        Process a media file (image or video) and extract face crops.\n",
        "\n",
        "        Args:\n",
        "            media_path (str): Path to the input media file.\n",
        "            output_dir (str): Directory to save face crops.\n",
        "            frame_id (str): Identifier for the frames.\n",
        "            frame_rate (int): Rate at which to extract frames (for videos).\n",
        "        \"\"\"\n",
        "        media_type = self.check_media_type(media_path)\n",
        "\n",
        "        if media_type == \"Image\":\n",
        "            self.process_image(\n",
        "                media_path,\n",
        "                output_dir,\n",
        "                frame_id=self.generate_combined_hash(media_path),\n",
        "                generate_crops_flag=generate_crops_flag,\n",
        "            )\n",
        "        elif media_type == \"Video\":\n",
        "            self.process_video(\n",
        "                media_path,\n",
        "                output_dir,\n",
        "                frame_id=self.generate_combined_hash(media_path),\n",
        "                frame_rate=frame_rate,\n",
        "                generate_crops_flag=generate_crops_flag,\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Unsupported media type: {media_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_SET_DIR = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/Datasets/spectrewolf8_celeb-df-v2-dataset~\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d897535e-8768-488c-a8b6-9727406409e3",
        "_uuid": "5acd0e91-2655-4697-af1f-366218ff438d",
        "execution": {
          "iopub.execute_input": "2025-01-24T12:25:32.302517Z",
          "iopub.status.busy": "2025-01-24T12:25:32.302147Z",
          "iopub.status.idle": "2025-01-24T12:25:33.175097Z",
          "shell.execute_reply": "2025-01-24T12:25:33.174191Z",
          "shell.execute_reply.started": "2025-01-24T12:25:32.302483Z"
        },
        "id": "iJmDC8WEenke",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ef6006e5-60c6-475c-8678-db73d176b3c6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "detector = MediaProcessor(threshold=0.4, log_level=1)\n",
        "media_path = f\"{DATA_SET_DIR}/Celeb-real/id0_0008.mp4\"\n",
        "output_dir = \"/kaggle/working/temp_dataset/real\"\n",
        "detector.process_media_file(media_path, output_dir, generate_crops_flag=False, frame_rate=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8ed96c9-3801-4918-b48e-e047658a8f14",
        "_uuid": "78b57e7f-6d1c-408b-9744-e14f58ec6114",
        "execution": {
          "iopub.execute_input": "2024-12-01T21:12:09.172972Z",
          "iopub.status.busy": "2024-12-01T21:12:09.172107Z",
          "iopub.status.idle": "2024-12-01T21:12:12.234093Z",
          "shell.execute_reply": "2024-12-01T21:12:12.23293Z",
          "shell.execute_reply.started": "2024-12-01T21:12:09.172932Z"
        },
        "id": "6e1WSk1Zenke",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !mkdir /kaggle/working/temp_face_crops\n",
        "# !mkdir /kaggle/working/temp_face_crops/real\n",
        "# !mkdir /kaggle/working/temp_face_crops/fake\n",
        "# !mkdir /kaggle/working/temp_dataset\n",
        "# !mkdir /kaggle/working/temp_dataset/real\n",
        "# !mkdir /kaggle/working/temp_dataset/fake\n",
        "\n",
        "\n",
        "# !rm -rf /kaggle/working/temp_face_crops\n",
        "# !rm -rf /kaggle/working/temp_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "232bf908-b805-4df1-92fc-80db5b4f7ce7",
        "_uuid": "c2de9eba-6b7d-4571-b5cb-da35b7861002",
        "id": "mEPrU9slenke",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for img_path in os.listdir(\"/kaggle/working/temp_face_crops\"):\n",
        "    # os.remove(os.path.join('/kaggle/working/temp_face_crops',img_path))\n",
        "    image = cv2.imread(os.path.join(\"/kaggle/working/temp_face_crops\", img_path))\n",
        "\n",
        "    # Convert the image from BGR to RGB color space\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis(\"off\")  # Turn off axis numbers\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "925ebdcc-a5e5-4460-accb-a1a183976d04",
        "_uuid": "edcdc899-c69a-4b9d-bcee-b3ab1ee54a6a",
        "execution": {
          "iopub.execute_input": "2024-12-01T20:38:02.422724Z",
          "iopub.status.busy": "2024-12-01T20:38:02.421969Z",
          "iopub.status.idle": "2024-12-01T20:38:02.501912Z",
          "shell.execute_reply": "2024-12-01T20:38:02.500774Z",
          "shell.execute_reply.started": "2024-12-01T20:38:02.422687Z"
        },
        "id": "HmhtzyQwenke",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Sample path to the image and output directory\n",
        "image_path = \"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_282.jpg\"  # Replace with your test image path\n",
        "frame_output_dir = \"/kaggle/working/temp_face_crops\"  # Directory to save the face crops\n",
        "shutil.rmtree(frame_output_dir)\n",
        "os.makedirs(frame_output_dir, exist_ok=True)\n",
        "\n",
        "# Read the image\n",
        "frame = cv2.imread(image_path)\n",
        "# # Resize the face crop to 224x224\n",
        "# frame = cv2.resize(frame, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "# Detect faces in the image\n",
        "detected_faces, face_found = detect_face(image_path=image_path, threshold=0.3, log_level=0)\n",
        "\n",
        "if face_found:\n",
        "    print(f\"{len(detected_faces)} faces detected in image: {image_path}\")\n",
        "    # Generate crops from detected faces\n",
        "    generate_crops(\n",
        "        frame=frame, frame_index=0, detected_faces=detected_faces, frame_id=\"frame\", show_crops=True\n",
        "    )\n",
        "else:\n",
        "    print(f\"No faces detected in image: {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66801e8d-fda4-41d1-8242-3dd0029d14b5",
        "_uuid": "0c360f72-3449-448b-9e0f-05cc2d9bde66",
        "execution": {
          "iopub.execute_input": "2024-12-01T17:05:43.99128Z",
          "iopub.status.busy": "2024-12-01T17:05:43.990862Z",
          "iopub.status.idle": "2024-12-01T17:05:45.179298Z",
          "shell.execute_reply": "2024-12-01T17:05:45.177691Z",
          "shell.execute_reply.started": "2024-12-01T17:05:43.991246Z"
        },
        "id": "Lbesvq-nenke",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print(len(os.listdir('/kaggle/working/temp/dataset/real')))\n",
        "# !mkdir '/kaggle/working/temp_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_SET_DIR = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/Datasets/spectrewolf8_celeb-df-v2-dataset~\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f76900f9-6311-4b48-882a-0a78e8af08cc",
        "_uuid": "0937eeab-3a11-4017-8c8e-c903a29c0709",
        "execution": {
          "iopub.execute_input": "2025-01-24T12:27:02.477321Z",
          "iopub.status.busy": "2025-01-24T12:27:02.476969Z",
          "iopub.status.idle": "2025-01-24T12:29:46.84938Z",
          "shell.execute_reply": "2025-01-24T12:29:46.848463Z",
          "shell.execute_reply.started": "2025-01-24T12:27:02.477292Z"
        },
        "id": "vMZAP48nenkf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "687ab801-6eb0-4ac9-c310-0b55e90b41b1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# RUN ONLY WHEN DATASET NEEDS TO BE CREATED\n",
        "\n",
        "# Generating CelebDF dataset\n",
        "detector = MediaProcessor(threshold=0.4, log_level=0)\n",
        "\n",
        "celeb_df_dataset = f\"{DATA_SET_DIR}\"\n",
        "\n",
        "output_dir = f\"{DATA_SET_DIR}/temp_dataset\"\n",
        "sample_size = 150\n",
        "\n",
        "celeb_df = os.listdir(celeb_df_dataset)\n",
        "celeb_df.sort()\n",
        "celeb_df_real = os.path.join(celeb_df_dataset, celeb_df[0])\n",
        "celeb_df_fake = os.path.join(celeb_df_dataset, celeb_df[1])\n",
        "\n",
        "print(celeb_df_dataset, celeb_df, celeb_df_real, celeb_df_fake)\n",
        "\n",
        "for media in os.listdir(celeb_df_real)[0:sample_size]:\n",
        "    media_path = os.path.join(celeb_df_real, media)\n",
        "    destination = os.path.join(output_dir, \"real\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=False, frame_rate=2)\n",
        "\n",
        "\n",
        "for media in os.listdir(celeb_df_fake)[0:sample_size]:\n",
        "    media_path = os.path.join(celeb_df_fake, media)\n",
        "    destination = os.path.join(output_dir, \"fake\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=False, frame_rate=2)\n",
        "    # print(os.path.join(celeb_df_real,vid_path))\n",
        "\n",
        "print(\"\\nDataset created\")\n",
        "\n",
        "# print(celeb_df_dataset, celeb_df_fake, celeb_df_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f8743c2-a96b-4458-9009-7934d75e2afd",
        "_uuid": "f6391601-4a62-4059-89c6-35f87f711dd1",
        "execution": {
          "iopub.execute_input": "2025-01-24T12:29:46.851921Z",
          "iopub.status.busy": "2025-01-24T12:29:46.851218Z",
          "iopub.status.idle": "2025-01-24T12:32:16.569264Z",
          "shell.execute_reply": "2025-01-24T12:32:16.568337Z",
          "shell.execute_reply.started": "2025-01-24T12:29:46.851875Z"
        },
        "id": "2JFiPcvXenkf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "199c8aaa-1628-4452-bf68-a83faf1768ee",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# RUN ONLY WHEN DATASET NEEDS TO BE CREATED\n",
        "\n",
        "# Generating CelebDF CROPS dataset\n",
        "detector = MediaProcessor(threshold=0.4, log_level=0)\n",
        "\n",
        "celeb_df_dataset = f\"{DATA_SET_DIR}\"\n",
        "output_dir = f\"{DATA_SET_DIR}/temp_crops_dataset\"  # maps onto dataset\n",
        "sample_size = 150\n",
        "\n",
        "celeb_df = os.listdir(celeb_df_dataset)\n",
        "celeb_df.sort()\n",
        "celeb_df_real = os.path.join(celeb_df_dataset, celeb_df[0])\n",
        "celeb_df_fake = os.path.join(celeb_df_dataset, celeb_df[1])\n",
        "\n",
        "print(celeb_df_dataset, celeb_df, celeb_df_real, celeb_df_fake)\n",
        "\n",
        "for media in os.listdir(celeb_df_real)[0:sample_size]:\n",
        "    media_path = os.path.join(celeb_df_real, media)\n",
        "    destination = os.path.join(output_dir, \"real\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=True, frame_rate=2)\n",
        "\n",
        "\n",
        "for media in os.listdir(celeb_df_fake)[0:sample_size]:\n",
        "    media_path = os.path.join(celeb_df_fake, media)\n",
        "    destination = os.path.join(output_dir, \"fake\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=True, frame_rate=2)\n",
        "    # print(os.path.join(celeb_df_real,vid_path))\n",
        "\n",
        "print(\"\\nCropped Dataset created\")\n",
        "\n",
        "# print(celeb_df_dataset, celeb_df_fake, celeb_df_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN ONLY WHEN DATASET NEEDS TO BE CREATED\n",
        "\n",
        "# Generating CelebDF CROPS dataset\n",
        "detector = MediaProcessor(threshold=0.4, log_level=0)\n",
        "\n",
        "saad_df_dataset = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/Datasets/Saad_custom-scraped-df-dataset/\"\n",
        "\n",
        "output_dir = f\"{DATA_SET_DIR}/temp_dataset\"  # maps onto dataset\n",
        "\n",
        "\n",
        "saad_df = os.listdir(saad_df_dataset)\n",
        "saad_df.sort()\n",
        "saad_df_fake = os.path.join(saad_df_dataset, saad_df[0])\n",
        "\n",
        "print(saad_df_dataset, saad_df, saad_df_fake)\n",
        "\n",
        "for media in os.listdir(saad_df_fake):\n",
        "    media_path = os.path.join(saad_df_fake, media)\n",
        "    destination = os.path.join(output_dir, \"fake\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=True, frame_rate=2)\n",
        "\n",
        "print(\"\\nDataset created\")\n",
        "\n",
        "output_dir = f\"{DATA_SET_DIR}/temp_crops_dataset\"  # maps onto dataset\n",
        "\n",
        "\n",
        "saad_df = os.listdir(saad_df_dataset)\n",
        "saad_df.sort()\n",
        "saad_df_fake = os.path.join(saad_df_dataset, saad_df[0])\n",
        "\n",
        "print(saad_df_dataset, saad_df, saad_df_fake)\n",
        "\n",
        "for media in os.listdir(saad_df_fake):\n",
        "    media_path = os.path.join(saad_df_fake, media)\n",
        "    destination = os.path.join(output_dir, \"fake\")\n",
        "    detector.process_media_file(media_path, destination, generate_crops_flag=True, frame_rate=2)\n",
        "\n",
        "print(\"\\nCropped Dataset created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5da31cea-b123-4fb2-b6e1-8b332c6c26ef",
        "_uuid": "a0545d93-0644-4b51-a58a-5752dc7ada4c",
        "collapsed": false,
        "id": "KDVhfP5wenkf",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "_cell_guid": "e216f38c-0924-4f17-9afd-78a43fa7e01b",
        "_uuid": "dd821870-f363-4108-a6b3-68eb55aaf99f",
        "execution": {
          "iopub.execute_input": "2025-01-24T18:15:30.246487Z",
          "iopub.status.busy": "2025-01-24T18:15:30.246003Z",
          "iopub.status.idle": "2025-01-24T18:15:30.250795Z",
          "shell.execute_reply": "2025-01-24T18:15:30.249911Z",
          "shell.execute_reply.started": "2025-01-24T18:15:30.246448Z"
        },
        "id": "NVK_8_63enkg",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_cell_guid": "7cde8cfe-6cef-44d7-8613-f9dab0766f6c",
        "_uuid": "f3ebcaac-ba67-4caf-ba52-138bc0a9dd03",
        "execution": {
          "iopub.execute_input": "2025-01-24T18:16:09.198084Z",
          "iopub.status.busy": "2025-01-24T18:16:09.197776Z",
          "iopub.status.idle": "2025-01-24T18:16:09.204408Z",
          "shell.execute_reply": "2025-01-24T18:16:09.203608Z",
          "shell.execute_reply.started": "2025-01-24T18:16:09.198057Z"
        },
        "id": "EtVDO18venkg",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, image_dir, cap=5000, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.samples_cap = cap\n",
        "\n",
        "        # Assuming there are two folders: 'real' and 'deepfake' in 'dataset' directory\n",
        "        for label in [\"real\", \"fake\"]:\n",
        "            label_dir = os.path.join(image_dir, label)\n",
        "            image_files = os.listdir(label_dir)\n",
        "            random.shuffle(image_files)\n",
        "            # Iterate over all images in 'real' and 'fake' directories\n",
        "            for idx, img_file in enumerate(image_files):\n",
        "                if idx > self.samples_cap:\n",
        "                    break\n",
        "                img_path = os.path.join(label_dir, img_file)\n",
        "                self.image_paths.append(img_path)\n",
        "\n",
        "                # Assign labels: 'real' -> 0, 'fake' -> 1\n",
        "                self.labels.append(0 if label == \"real\" else 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # try:\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image and convert it to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply the transform, if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error loading image {img_path}: {e}\")\n",
        "        #     image = Image.new('RGB', (224, 224), color='black')\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "_cell_guid": "c5a6444e-94d0-4331-94b2-42b2ac019c4d",
        "_uuid": "e202b917-e661-4a2a-8942-4ca476d57ccd",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:00.376051Z",
          "iopub.status.busy": "2025-01-24T20:23:00.37574Z",
          "iopub.status.idle": "2025-01-24T20:23:00.398477Z",
          "shell.execute_reply": "2025-01-24T20:23:00.397525Z",
          "shell.execute_reply.started": "2025-01-24T20:23:00.376028Z"
        },
        "id": "FWUYJw7qenkg",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "be08feb8-3301-4b8f-cf2a-9bd23e73823f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 8501\n",
            "Validation dataset size: 1500\n",
            "Test dataset size: 1\n",
            "Train loader: <torch.utils.data.dataloader.DataLoader object at 0x7b94f0b11640>\n",
            "Validation loader: <torch.utils.data.dataloader.DataLoader object at 0x7b94ae3007d0>\n",
            "Test loader: <torch.utils.data.dataloader.DataLoader object at 0x7b94ae330260>\n"
          ]
        }
      ],
      "source": [
        "# /kaggle/working/temp_2/dataset/\n",
        "#  real/\n",
        "#  fake/\n",
        "\n",
        "# Define the dataset with all data\n",
        "dataset = DeepfakeDataset(image_dir=f\"{DATA_SET_DIR}/temp_crops_dataset\", transform=transform)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Print details for confirmation\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Train loader: {train_loader}\")\n",
        "print(f\"Validation loader: {val_loader}\")\n",
        "print(f\"Test loader: {test_loader}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5818da76-7e84-4cd7-9114-abf4fa25e847",
        "_uuid": "eb4d9d24-296a-42db-8379-78c24c0f954c",
        "collapsed": false,
        "id": "X3tHq3L1enkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Core Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "_cell_guid": "8b11f1c2-05bd-46ac-85f5-e2013f247df0",
        "_uuid": "63a58c26-e2f8-49c1-8710-ae5c7325c20f",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:10.093559Z",
          "iopub.status.busy": "2025-01-24T20:23:10.093236Z",
          "iopub.status.idle": "2025-01-24T20:23:11.57759Z",
          "shell.execute_reply": "2025-01-24T20:23:11.576626Z",
          "shell.execute_reply.started": "2025-01-24T20:23:10.093529Z"
        },
        "id": "uvJvvuJZenkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from torchvision.models import ResNeXt101_32X8D_Weights, ResNeXt101_64X4D_Weights, ResNext\n",
        "\n",
        "# # model = models.resnext50_32x4d()\n",
        "# # model = models.resnext101_32x8d()\n",
        "# # model = models.resnext101_64x4d()\n",
        "\n",
        "# model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.DEFAULT)\n",
        "\n",
        "# # Modify the last layer to match the number of classes (real or fake)\n",
        "# num_ftrs = model.fc.in_features\n",
        "# model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: real and fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /home/b450-plus/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|| 77.4M/77.4M [00:34<00:00, 2.35MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import (\n",
        "    densenet201,\n",
        "    DenseNet201_Weights\n",
        ")\n",
        "\n",
        "# model = models.resnext50_32x4d()\n",
        "# model = models.resnext101_32x8d()\n",
        "# model = models.resnext101_64x4d()\n",
        "\n",
        "model = densenet201(weights=DenseNet201_Weights.DEFAULT)\n",
        "\n",
        "# Modify the last layer to match the number of classes (real or fake)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 2)  # 2 classes: real and fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "_cell_guid": "2f6dddf4-aa57-4be9-ac37-509a13356885",
        "_uuid": "84a1b607-f6de-45f6-b31f-a07c6cdd8008",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:11.579064Z",
          "iopub.status.busy": "2025-01-24T20:23:11.5788Z",
          "iopub.status.idle": "2025-01-24T20:23:11.587816Z",
          "shell.execute_reply": "2025-01-24T20:23:11.58698Z",
          "shell.execute_reply.started": "2025-01-24T20:23:11.579042Z"
        },
        "id": "JKy0_j4Henkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "_cell_guid": "3c9266f7-cc90-4260-8b4b-36b313617d85",
        "_uuid": "337293b7-cbe4-40ea-889e-441c35a0b6f0",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:12.04705Z",
          "iopub.status.busy": "2025-01-24T20:23:12.046759Z",
          "iopub.status.idle": "2025-01-24T20:23:12.153213Z",
          "shell.execute_reply": "2025-01-24T20:23:12.15236Z",
          "shell.execute_reply.started": "2025-01-24T20:23:12.047026Z"
        },
        "id": "tJu7ULCkenkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "d5deaa4d-49f4-426c-9119-14b59e0c1d63",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "# print(device,'\\n', model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "_cell_guid": "fc1e42c8-e86e-4f4a-a6b8-3f8cfb3c93a7",
        "_uuid": "3054584c-9c4e-4a18-8420-198aac2ff4d9",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:16.032514Z",
          "iopub.status.busy": "2025-01-24T20:23:16.03221Z",
          "iopub.status.idle": "2025-01-24T20:23:16.042745Z",
          "shell.execute_reply": "2025-01-24T20:23:16.04186Z",
          "shell.execute_reply.started": "2025-01-24T20:23:16.032491Z"
        },
        "id": "VoRAwq2venkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize metrics\n",
        "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "precision_metric = Precision(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "recall_metric = Recall(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "f1_metric = F1Score(task=\"multiclass\", num_classes=2, average=\"macro\").to(device)\n",
        "confusion_matrix_metric = ConfusionMatrix(task=\"multiclass\", num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd27ce35-75ff-409e-a50d-d405835621a5",
        "_uuid": "9b8e2e83-f8b9-46be-b648-1330aa80c633",
        "collapsed": false,
        "id": "DzIm-r5Cenkh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Train loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:18.583778Z",
          "iopub.status.busy": "2025-01-24T20:23:18.583487Z",
          "iopub.status.idle": "2025-01-24T20:23:18.587367Z",
          "shell.execute_reply": "2025-01-24T20:23:18.586364Z",
          "shell.execute_reply.started": "2025-01-24T20:23:18.583753Z"
        },
        "id": "x4THxr1yenkh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from PIL import Image, ImageFile\n",
        "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# # In your dataset class\n",
        "# def __getitem__(self, idx):\n",
        "#     img_path = self.image_paths[idx]\n",
        "#     image = Image.open(img_path).convert('RGB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "_cell_guid": "e0a5d360-5b59-4345-8734-a2df8388ea2f",
        "_uuid": "c1ad90ef-40c0-4880-a601-61764aefa8f8",
        "execution": {
          "iopub.execute_input": "2025-01-24T20:23:19.200721Z",
          "iopub.status.busy": "2025-01-24T20:23:19.200436Z",
          "iopub.status.idle": "2025-01-24T21:10:31.486944Z",
          "shell.execute_reply": "2025-01-24T21:10:31.486087Z",
          "shell.execute_reply.started": "2025-01-24T20:23:19.200698Z"
        },
        "id": "9ifdMa6Benki",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "d3f90a05-cce1-470c-85e8-a80ddb14b475",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/b450-plus/snap/code/181/.local/share/virtualenvs/DMI_FYP_dj_primary-backend-cvNQsyiC/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------\n",
            "Epoch [1/3]\n",
            "Train Loss: 0.2174\n",
            "Train Metrics:\n",
            "  Accuracy: 91.15%\n",
            "  Precision: 0.91\n",
            "  Recall: 0.91\n",
            "  F1 Score: 0.91\n",
            "--------\n",
            "Epoch [2/3]\n",
            "Train Loss: 0.1527\n",
            "Train Metrics:\n",
            "  Accuracy: 93.79%\n",
            "  Precision: 0.94\n",
            "  Recall: 0.94\n",
            "  F1 Score: 0.94\n",
            "--------\n",
            "Epoch [3/3]\n",
            "Train Loss: 0.1293\n",
            "Train Metrics:\n",
            "  Accuracy: 94.80%\n",
            "  Precision: 0.95\n",
            "  Recall: 0.95\n",
            "  F1 Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Reset metrics at start of epoch\n",
        "    accuracy_metric.reset()\n",
        "    precision_metric.reset()\n",
        "    recall_metric.reset()\n",
        "    f1_metric.reset()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = accuracy_metric.compute() * 100\n",
        "    epoch_precision = precision_metric.compute()\n",
        "    epoch_recall = recall_metric.compute()\n",
        "    epoch_f1 = f1_metric.compute()\n",
        "\n",
        "    print(\n",
        "        f\"--------\\nEpoch [{epoch + 1}/{num_epochs}]\"\n",
        "        f\"\\nTrain Loss: {epoch_loss:.4f}\"\n",
        "        f\"\\nTrain Metrics:\"\n",
        "        f\"\\n  Accuracy: {epoch_acc:.2f}%\"\n",
        "        f\"\\n  Precision: {epoch_precision:.2f}\"\n",
        "        f\"\\n  Recall: {epoch_recall:.2f}\"\n",
        "        f\"\\n  F1 Score: {epoch_f1:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4b2e1c9e-1770-466c-904f-a153519db67f",
        "_uuid": "7b917fdc-8499-456a-bf8d-a644054d8e28",
        "collapsed": false,
        "id": "pmiWFgAzenki",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Val loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "_cell_guid": "79a700b0-6fc6-486b-9e1f-8550888707c3",
        "_uuid": "a35d57a4-ac50-4c30-988c-660dbf2b7645",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:10:31.488679Z",
          "iopub.status.busy": "2025-01-24T21:10:31.488388Z",
          "iopub.status.idle": "2025-01-24T21:10:44.082062Z",
          "shell.execute_reply": "2025-01-24T21:10:44.081203Z",
          "shell.execute_reply.started": "2025-01-24T21:10:31.488656Z"
        },
        "id": "KLqBxKADenki",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "7522031a-2a60-4806-a421-de6e3a66c88e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.1475, Accuracy: 93.80%\n",
            "Precision: 0.94, Recall: 0.94, F1 Score: 0.94\n",
            "Confusion Matrix:\n",
            "[[686  69]\n",
            " [ 24 721]]\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "val_loss = 0.0\n",
        "\n",
        "# Reset all metrics at the start of validation\n",
        "accuracy_metric.reset()\n",
        "precision_metric.reset()\n",
        "recall_metric.reset()\n",
        "f1_metric.reset()\n",
        "confusion_matrix_metric.reset()\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "        confusion_matrix_metric.update(outputs, labels)\n",
        "\n",
        "# Compute final metrics\n",
        "val_loss = val_loss / len(val_loader)\n",
        "val_accuracy = accuracy_metric.compute() * 100\n",
        "val_precision = precision_metric.compute()\n",
        "val_recall = recall_metric.compute()\n",
        "val_f1 = f1_metric.compute()\n",
        "val_confusion_matrix = confusion_matrix_metric.compute()\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
        "print(f\"Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{val_confusion_matrix.cpu().numpy()}\")\n",
        "\n",
        "# Confusion matrix\n",
        "# [[TN  FP]\n",
        "#  [FN  TP]]\n",
        "\n",
        "# TN = True Negatives\n",
        "# FP = False Positives\n",
        "# FN = False Negatives\n",
        "# TP = True Positives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e61294d8-c25c-4453-89e7-79c31a703188",
        "_uuid": "c805b340-3837-4789-a8be-9f5983ba7477",
        "collapsed": false,
        "id": "GUXk1-IDenki",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Test loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c805fec0-da35-4f85-bf34-2e5862d44c5e",
        "_uuid": "3c9d1848-7adc-4c70-94fe-58f8078c2152",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:10:44.083605Z",
          "iopub.status.busy": "2025-01-24T21:10:44.083361Z",
          "iopub.status.idle": "2025-01-24T21:10:44.123009Z",
          "shell.execute_reply": "2025-01-24T21:10:44.122381Z",
          "shell.execute_reply.started": "2025-01-24T21:10:44.083584Z"
        },
        "id": "5-1CxXDsenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "1b24229c-2a91-4493-c97c-0176ae655120",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "\n",
        "# Reset all metrics before testing\n",
        "accuracy_metric.reset()\n",
        "precision_metric.reset()\n",
        "recall_metric.reset()\n",
        "f1_metric.reset()\n",
        "confusion_matrix_metric.reset()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Update metrics batch by batch\n",
        "        accuracy_metric.update(outputs, labels)\n",
        "        precision_metric.update(outputs, labels)\n",
        "        recall_metric.update(outputs, labels)\n",
        "        f1_metric.update(outputs, labels)\n",
        "        confusion_matrix_metric.update(outputs, labels)\n",
        "\n",
        "# Compute final metrics\n",
        "test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = accuracy_metric.compute() * 100\n",
        "test_precision = precision_metric.compute()\n",
        "test_recall = recall_metric.compute()\n",
        "test_f1 = f1_metric.compute()\n",
        "test_confusion_matrix = confusion_matrix_metric.compute()\n",
        "\n",
        "print(\n",
        "    f\"Test Metrics:\"\n",
        "    f\"\\n  Loss: {test_loss:.4f}\"\n",
        "    f\"\\n  Accuracy: {test_accuracy:.2f}%\"\n",
        "    f\"\\n  Precision: {test_precision:.2f}\"\n",
        "    f\"\\n  Recall: {test_recall:.2f}\"\n",
        "    f\"\\n  F1 Score: {test_f1:.2f}\"\n",
        ")\n",
        "print(f\"Confusion Matrix:\\n{test_confusion_matrix.cpu().numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d9abc6fe-c680-49fb-a82a-74deec004e47",
        "_uuid": "b95c651a-9497-4d02-aed8-9820a6e59065",
        "collapsed": false,
        "id": "_OGyHl10enkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Exporting model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a38cf39b-2562-4707-bfca-20c35745501b",
        "_uuid": "2cd8a818-00f4-467e-a0ce-4d892e4dd932",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:14:30.042842Z",
          "iopub.status.busy": "2025-01-24T21:14:30.042531Z",
          "iopub.status.idle": "2025-01-24T21:14:30.554541Z",
          "shell.execute_reply": "2025-01-24T21:14:30.553736Z",
          "shell.execute_reply.started": "2025-01-24T21:14:30.04282Z"
        },
        "id": "Z1KdCqwwenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), '/kaggle/working/model/acc99.49_test-1_deepfake_detector_resnext50.pth')\n",
        "model_path = f\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/DMI_backend/ML_Models/V3_CROPS_deepfake_detector_SWIN_V2_B_acc{val_accuracy:.2f}_epochs{num_epochs}.pth\"\n",
        "torch.save(model, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting to Hugging_face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project root directory to Python path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# import the helper\n",
        "from Hugging_face_helper.helper.main import HuggingFaceHelper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "hf_helper = HuggingFaceHelper(\n",
        "    token=hf_token,\n",
        "    repo_name=\"spectrewolf8/DMI_FYP_Models_Repo\",\n",
        "    repo_local_dir=\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/Hugging_face_helper/repo/\",\n",
        "    cache_dir=\"/home/b450-plus/DMI_FYP_dj_primary-backend/DMI_FYP_dj_primary-backend/Hugging_face_helper/cache/\",\n",
        ")\n",
        "\n",
        "local_model_path = model_path\n",
        "filename = model_path.split(\"/\")[-1]\n",
        "\n",
        "# Upload the model\n",
        "hf_helper.upload_model(local_model_path)\n",
        "\n",
        "# Download the model back\n",
        "downloaded_model = hf_helper.download_model(filename)\n",
        "print(\"Model downloaded to:\", downloaded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f047a22e-e22c-41de-b803-59cdc05de2c3",
        "_uuid": "7f838ac4-ca81-4e1a-ac3d-d0971e580d51",
        "collapsed": false,
        "id": "rwQ1DCBSenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Manual testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b5d78c0-ae47-4b18-8548-8cb15b7c4906",
        "_uuid": "c4b4735d-de38-4a07-ab1b-c1ff398e95ad",
        "id": "Obd-PEOrenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Assuming your model is already defined and loaded\n",
        "# # model = models.resnext50_32x4d()  # Replace with your actual model definition\n",
        "# model = torch.load('/kaggle/working/model/acc99.49_test-1_deepfake_detector_resnext50.pth',weights_only=True)  # Load the trained model weights\n",
        "# model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# # If using GPU\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79d9d6e6-f82c-4e12-a5e2-f044fb0042ba",
        "_uuid": "21fe590c-6024-4ba1-95e9-a6f5c1d678dd",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:14:45.925519Z",
          "iopub.status.busy": "2025-01-24T21:14:45.925192Z",
          "iopub.status.idle": "2025-01-24T21:14:45.930839Z",
          "shell.execute_reply": "2025-01-24T21:14:45.93005Z",
          "shell.execute_reply.started": "2025-01-24T21:14:45.92549Z"
        },
        "id": "7vZ7KBvYenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_test_image(image_path, show_image=False):\n",
        "    if show_image:\n",
        "        cv_img = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "        cv_img_rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the image with detected faces\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(cv_img_rgb)\n",
        "        plt.axis(\"off\")  # Hide axis\n",
        "        plt.title(f\"Test image\")\n",
        "        plt.show()\n",
        "\n",
        "    #     Define the transformations (should be the same as used in training)\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((224, 224)),  # Resize the image to the input size of the model\n",
        "            transforms.ToTensor(),  # Convert image to tensor\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "            ),  # Normalize as per the pre-trained model's requirements\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply the transformations\n",
        "    image = transform(image)\n",
        "\n",
        "    # Add a batch dimension (models expect a batch of images, even if it's just one image)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "_cell_guid": "622cff59-27e9-43c0-9ffa-a7b40dbfa76b",
        "_uuid": "d591a0c5-05b6-479e-929f-c1acb9e9e89f",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:14:54.591842Z",
          "iopub.status.busy": "2025-01-24T21:14:54.59156Z",
          "iopub.status.idle": "2025-01-24T21:14:54.863836Z",
          "shell.execute_reply": "2025-01-24T21:14:54.863157Z",
          "shell.execute_reply.started": "2025-01-24T21:14:54.591819Z"
        },
        "id": "5z5g_IKEenkj",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "8d443f6a-d2b1-43bb-ba74-f342ee206500",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@1975.318] global loadsave.cpp:241 findDecoder imread_('/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_1235.jpg'): can't open/read file: check file path/integrity\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_1235.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_test_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to compute gradients for inference\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[35], line 6\u001b[0m, in \u001b[0;36mload_test_image\u001b[0;34m(image_path, show_image)\u001b[0m\n\u001b[1;32m      3\u001b[0m cv_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert the image from BGR to RGB for displaying with matplotlib\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m cv_img_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the image with detected faces\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "image_path = \"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_1235.jpg\"\n",
        "image = load_test_image(image_path=image_path, show_image=True)\n",
        "# Make prediction\n",
        "with torch.no_grad():  # No need to compute gradients for inference\n",
        "    image = image.to(device)\n",
        "    output = model(image)  # Forward pass\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = tnf.softmax(output, dim=1)\n",
        "\n",
        "    # Get the class with the highest probability\n",
        "    confidence, predicted = torch.max(probabilities, 1)  # Get the class with highest probability\n",
        "\n",
        "# Convert the prediction to a label (assuming you have a mapping of class indices to labels)\n",
        "label_map = {0: \"real\", 1: \"fake\"}  # Adjust based on your dataset\n",
        "predicted_label = label_map[predicted.item()]\n",
        "\n",
        "# Print predicted label and confidence score\n",
        "confidence_score = confidence.item()\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "print(f\"Confidence score: {confidence_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "277138ae-1350-4d27-be2f-af1a8dcea427",
        "_uuid": "1dc1a61b-6025-4452-a412-09d7c8b54e06",
        "collapsed": false,
        "id": "8ZusB0-Zenkk",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "source": [
        "## Loading Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6bba9282-f436-4170-8625-b3bef1447ec5",
        "_uuid": "b96ae455-eca2-43d7-93f7-1a41254b99ce",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:15:14.061819Z",
          "iopub.status.busy": "2025-01-24T21:15:14.0615Z",
          "iopub.status.idle": "2025-01-24T21:15:14.373649Z",
          "shell.execute_reply": "2025-01-24T21:15:14.372799Z",
          "shell.execute_reply.started": "2025-01-24T21:15:14.06179Z"
        },
        "id": "EhfF5WwNenkk",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "f4867a34-60a2-46fd-fa19-e3629a52e28e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Assuming your model is already defined and loaded\n",
        "# model = models.resnext50_32x4d()  # Replace with your actual model definition\n",
        "loaded_model = torch.load(\n",
        "    downloaded_model,\n",
        "    map_location=\"cpu\",\n",
        "    weights_only=False,\n",
        ")  # Load the trained model weights\n",
        "loaded_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# If using GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfc8cc1e-30a6-4ab9-b69a-5876ddbda0a6",
        "_uuid": "708c9f73-8a43-414f-b430-401e94cf0eec",
        "execution": {
          "iopub.execute_input": "2025-01-24T21:16:08.24731Z",
          "iopub.status.busy": "2025-01-24T21:16:08.246949Z",
          "iopub.status.idle": "2025-01-24T21:16:14.768823Z",
          "shell.execute_reply": "2025-01-24T21:16:14.768137Z",
          "shell.execute_reply.started": "2025-01-24T21:16:08.24728Z"
        },
        "id": "GuW0r8g4enkk",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "11b3ff0a-8373-4f6e-db06-1ba56689aaf8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_dir = \"/home/b450-plus/DMI_FYP_dj_primary-backend/Datasets/spectrewolf8_celeb-df-v2-dataset~/temp_dataset/real\"\n",
        "images = os.listdir(image_dir)\n",
        "for image_file in images[0:5]:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    image = load_test_image(image_path=image_path, show_image=True)\n",
        "    # Make prediction\n",
        "    with torch.no_grad():  # No need to compute gradients for inference\n",
        "        image = image.to(device)\n",
        "        output = loaded_model(image)  # Forward pass\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = tnf.softmax(output, dim=1)\n",
        "\n",
        "        # Get the class with the highest probability\n",
        "        confidence, predicted = torch.max(probabilities, 1)  # Get the class with highest probability\n",
        "\n",
        "    # Convert the prediction to a label (assuming you have a mapping of class indices to labels)\n",
        "    label_map = {0: \"real\", 1: \"fake\"}  # Adjust based on your dataset\n",
        "    predicted_label = label_map[predicted.item()]\n",
        "\n",
        "    # Print predicted label and confidence score\n",
        "    confidence_score = confidence.item()\n",
        "    print(f\"Predicted label: {predicted_label}\")\n",
        "    print(f\"Confidence score: {confidence_score*100:.2f}\")\n",
        "\n",
        "# showing example image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-13T19:28:30.633975Z",
          "iopub.status.busy": "2024-12-13T19:28:30.633544Z",
          "iopub.status.idle": "2024-12-13T19:28:30.675691Z",
          "shell.execute_reply": "2024-12-13T19:28:30.674291Z",
          "shell.execute_reply.started": "2024-12-13T19:28:30.633941Z"
        },
        "id": "v8-r-IAQenkk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DeepfakeDetectionPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        frame_model_path,\n",
        "        crop_model_path,\n",
        "        frames_dir,\n",
        "        crops_dir,\n",
        "        threshold=0.4,\n",
        "        log_level=0,\n",
        "        FRAMES_FILE_FORMAT=\"jpg\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline with both frame and crop models.\n",
        "\n",
        "        Args:\n",
        "            frame_model_path (str): Path to the frame analysis model\n",
        "            crop_model_path (str): Path to the crop analysis model\n",
        "            frames_dir (str): Directory containing all frames\n",
        "            crops_dir (str): Directory containing all crops\n",
        "            threshold (float): Confidence threshold for face detection\n",
        "            log_level (int): Logging verbosity level\n",
        "        \"\"\"\n",
        "        # Load models and move to appropriate device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.frame_model = torch.load(frame_model_path, map_location=self.device)\n",
        "        self.crop_model = torch.load(crop_model_path, map_location=self.device)\n",
        "        self.frame_model.eval()\n",
        "        self.crop_model.eval()\n",
        "\n",
        "        # Set up image transformation\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.log_level = log_level\n",
        "        self.label_map = {0: \"real\", 1: \"fake\"}\n",
        "        self.frames_dir = frames_dir\n",
        "        self.crops_dir = crops_dir\n",
        "        self.FRAMES_FILE_FORMAT = FRAMES_FILE_FORMAT\n",
        "\n",
        "        # Initialize MediaProcessor for face detection\n",
        "        self.media_processor = MediaProcessor(\n",
        "            threshold=threshold, log_level=log_level, FRAMES_FILE_FORMAT=self.FRAMES_FILE_FORMAT\n",
        "        )\n",
        "\n",
        "    def get_crops_for_frame(self, file_id, frame_index, crops_dir):\n",
        "        \"\"\"\n",
        "        Get all crops belonging to a specific frame using the naming scheme.\n",
        "\n",
        "        Args:\n",
        "            file_id (str): Combined hash identifier (content_hash + name_hash)\n",
        "            frame_index (int): Frame index (0 for images)\n",
        "            crops_dir (str): Directory containing all crops\n",
        "\n",
        "        Returns:\n",
        "            list: Paths to relevant crop files\n",
        "        \"\"\"\n",
        "        crop_prefix = f\"{file_id}_{frame_index}_\"\n",
        "        relevant_crops = []\n",
        "\n",
        "        for filename in os.listdir(crops_dir):\n",
        "            if filename.startswith(crop_prefix):\n",
        "                crop_path = os.path.join(crops_dir, filename)\n",
        "                relevant_crops.append(crop_path)\n",
        "\n",
        "        return natsorted(relevant_crops)  # Sort to ensure consistent ordering\n",
        "\n",
        "    def load_image_preprocessed(self, image_path, show_image=False):\n",
        "        if show_image:\n",
        "            cv_img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "            cv_img_rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Display the image with detected faces\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(cv_img_rgb)\n",
        "            plt.axis(\"off\")  # Hide axis\n",
        "            plt.title(f\"Test image\")\n",
        "            plt.show()\n",
        "\n",
        "        #  Define the transformations (should be the same as used in training)\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),  # Resize the image to the input size of the model\n",
        "                transforms.ToTensor(),  # Convert image to tensor\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),  # Normalize as per the pre-trained model's requirements\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply the transformations\n",
        "        image = transform(image)\n",
        "\n",
        "        # Add a batch dimension (models expect a batch of images, even if it's just one image)\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def process_frame(self, image_path, type=\"frame\"):\n",
        "        \"\"\"\n",
        "        Process a single frame through frame-level model with integrated GradCAM for frames only.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "            type (str): Type of processing, either \"frame\" or \"crop\"\n",
        "\n",
        "        Returns:\n",
        "            tuple: (predicted_label, confidence_score, gradcam_path if type==\"frame\" else None)\n",
        "        \"\"\"\n",
        "        if type not in [\"frame\", \"crop\"]:\n",
        "            raise ValueError(\"Invalid type. Expected 'frame' or 'crop'.\")\n",
        "\n",
        "        model = self.frame_model if type == \"frame\" else self.crop_model\n",
        "        gradcam_path = None\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = self.load_image_preprocessed(image_path, show_image=False)\n",
        "        image = image.to(self.device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            probabilities = tnf.softmax(output, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "            predicted_class = predicted.item()\n",
        "\n",
        "        predicted_label = self.label_map[predicted_class]\n",
        "        confidence_score = confidence.item()\n",
        "\n",
        "        # Generate GradCAM only for frame-level analysis\n",
        "        if type == \"frame\":\n",
        "            gradcam_path = image_path.replace(\n",
        "                f\".{self.FRAMES_FILE_FORMAT}\", f\"_gradcam.{self.FRAMES_FILE_FORMAT}\"\n",
        "            )\n",
        "\n",
        "            # Target the last convolutional layer\n",
        "            target_layers = [model.layer4[-1]]\n",
        "\n",
        "            # Create GradCAM object\n",
        "            cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "            # Define target for GradCAM\n",
        "            targets = [ClassifierOutputTarget(predicted_class)]\n",
        "\n",
        "            # Generate grayscale CAM\n",
        "            grayscale_cam = cam(input_tensor=image, targets=targets)\n",
        "            grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "            # Load and prepare original image for overlay\n",
        "            rgb_img = cv2.imread(image_path)\n",
        "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
        "            # rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB) #convert BGR to RGB color space\n",
        "            rgb_img = rgb_img / 255.0\n",
        "\n",
        "            # Create visualization\n",
        "            visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "            # visualization = cv2.cvtColor(visualization, cv2.COLOR_BGR2RGB) #convert BGR to RGB color space\n",
        "\n",
        "            # plt.imshow(visualization)\n",
        "\n",
        "            # Save visualization\n",
        "            cv2.imwrite(gradcam_path, visualization)\n",
        "\n",
        "            if self.log_level >= 2:\n",
        "                print(f\"GradCAM saved to: {gradcam_path}\")\n",
        "\n",
        "        if self.log_level >= 2:\n",
        "            print(f\"Type: {type}\")\n",
        "            print(f\"Predicted label: {predicted_label}\")\n",
        "            print(f\"Confidence score: {confidence_score*100:.2f}\")\n",
        "\n",
        "        return predicted_label, confidence_score, gradcam_path if type == \"frame\" else None\n",
        "\n",
        "    def analyze_frame_with_crops(self, image_path, frame_id):\n",
        "        \"\"\"\n",
        "        Analyze a frame both at frame-level and crop-level.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "            frame_id (str): Identifier for the frame\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results including frame and crop predictions\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"frame_id\": frame_id,\n",
        "            \"frame_analysis\": None,\n",
        "            \"crop_analyses\": [],\n",
        "            \"final_verdict\": None,\n",
        "            \"frame_path\": image_path,\n",
        "            \"crop_paths\": [],\n",
        "            \"ela_path\": None,\n",
        "            \"gradcam_path\": None,\n",
        "        }\n",
        "\n",
        "        # Frame-level analysis with GradCAM\n",
        "        frame_pred, frame_conf, gradcam_path = self.process_frame(image_path, type=\"frame\")\n",
        "        results[\"frame_analysis\"] = {\"prediction\": frame_pred, \"confidence\": frame_conf}\n",
        "        results[\"gradcam_path\"] = gradcam_path\n",
        "\n",
        "        # Get crops for this frame\n",
        "        frame_index = 0 if \"_\" not in frame_id else int(frame_id.split(\"_\")[-1])\n",
        "        file_id = frame_id.rsplit(\"_\", 1)[0]\n",
        "        crop_paths = self.get_crops_for_frame(file_id, frame_index, self.crops_dir)\n",
        "\n",
        "        # Analyze each crop (without GradCAM)\n",
        "        for crop_path in crop_paths:\n",
        "            crop_pred, crop_conf, _ = self.process_frame(crop_path, type=\"crop\")\n",
        "\n",
        "            crop_index = int(os.path.splitext(crop_path)[0].split(\"_\")[-1])\n",
        "\n",
        "            results[\"crop_analyses\"].append(\n",
        "                {\n",
        "                    \"face_index\": crop_index,\n",
        "                    \"prediction\": crop_pred,\n",
        "                    \"confidence\": crop_conf,\n",
        "                }\n",
        "            )\n",
        "            results[\"crop_paths\"].append(crop_path)\n",
        "\n",
        "        # Determine final verdict\n",
        "        if len(results[\"crop_analyses\"]) > 0:\n",
        "            frame_is_fake = frame_pred == \"fake\"\n",
        "            crops_fake_count = sum(\n",
        "                1 for crop in results[\"crop_analyses\"] if crop[\"prediction\"] == \"fake\"\n",
        "            )\n",
        "            crops_total = len(results[\"crop_analyses\"])\n",
        "\n",
        "            if frame_is_fake and crops_fake_count > crops_total / 2:\n",
        "                results[\"final_verdict\"] = \"fake\"\n",
        "            else:\n",
        "                results[\"final_verdict\"] = \"real\"\n",
        "        else:\n",
        "            results[\"final_verdict\"] = frame_pred\n",
        "\n",
        "        # Perform ELA analysis\n",
        "        results[\"ela_path\"] = self.perform_ela_analysis(image_path)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def perform_ela_analysis(self, image_path):\n",
        "        \"\"\"\n",
        "        Perform Error Level Analysis (ELA) on the image.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the ELA image\n",
        "        \"\"\"\n",
        "        ela_image_path = image_path.replace(\n",
        "            f\".{self.FRAMES_FILE_FORMAT}\", f\"_ela.{self.FRAMES_FILE_FORMAT}\"\n",
        "        )\n",
        "\n",
        "        # # Skip if ELA image already exists\n",
        "        # if os.path.exists(ela_image_path):\n",
        "        #     if self.log_level >= 1:\n",
        "        #         print(f\"Skipping frame {ela_image_path}: already exists\")\n",
        "        #     return ela_image_path\n",
        "\n",
        "        # Open image using PIL\n",
        "        original_image = Image.open(image_path)\n",
        "        temp_compressed = os.path.join(os.path.dirname(image_path), \"temp_compressed.jpg\")\n",
        "\n",
        "        # Save compressed version\n",
        "        quality = 95\n",
        "        scale_multiplier = 50\n",
        "        original_image.save(temp_compressed, \"JPEG\", quality=quality)\n",
        "        compressed_image = Image.open(temp_compressed)\n",
        "\n",
        "        # Calculate difference\n",
        "        ela_image = ImageChops.difference(original_image, compressed_image)\n",
        "\n",
        "        # Apply noise boost and scaling\n",
        "        ela_scaled = ela_image.point(lambda x: np.sign(x) * (np.abs(x) ** 1.5 * scale_multiplier))\n",
        "\n",
        "        # Save ELA image\n",
        "        ela_scaled.save(ela_image_path)\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_compressed):\n",
        "            os.remove(temp_compressed)\n",
        "\n",
        "        return ela_image_path\n",
        "\n",
        "    def process_media(self, media_path, frame_rate=2):\n",
        "        \"\"\"\n",
        "        Process a media file (image or video) through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            media_path (str): Path to the media file\n",
        "            frame_rate (int): Frame rate for video processing\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results for the entire media file\n",
        "        \"\"\"\n",
        "        media_type = self.media_processor.check_media_type(media_path)\n",
        "\n",
        "        # Generate file identifier using MediaProcessor's hash functions\n",
        "        file_id = self.media_processor.generate_combined_hash(media_path)\n",
        "\n",
        "        # Process media file for frames using MediaProcessor\n",
        "        self.media_processor.process_media_file(\n",
        "            media_path, self.frames_dir, generate_crops_flag=False, frame_rate=frame_rate\n",
        "        )\n",
        "\n",
        "        # Process media file for crops using MediaProcessor\n",
        "        self.media_processor.process_media_file(\n",
        "            media_path, self.crops_dir, generate_crops_flag=True, frame_rate=frame_rate\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            \"media_path\": media_path,\n",
        "            \"media_type\": media_type,\n",
        "            \"file_id\": file_id,\n",
        "            \"frame_results\": [],\n",
        "        }\n",
        "\n",
        "        if media_type == \"Image\":\n",
        "            media_path = os.path.join(self.frames_dir, f\"{file_id}_0.{self.FRAMES_FILE_FORMAT}\")\n",
        "            frame_results = self.analyze_frame_with_crops(media_path, f\"{file_id}_0\")\n",
        "            results[\"media_path\"] = media_path\n",
        "            results[\"frame_results\"].append(frame_results)\n",
        "\n",
        "        elif media_type == \"Video\":\n",
        "            frames = [\n",
        "                os.path.join(self.frames_dir, f)\n",
        "                for f in os.listdir(self.frames_dir)\n",
        "                if (f.startswith(file_id) and (\"ela\" not in f and \"gradcam\" not in f))\n",
        "            ]\n",
        "            frames = natsorted(frames)\n",
        "            for frame_index, frame_path in enumerate(frames):\n",
        "                # print(\"ID/PATH: \",frame_index, frame_path)\n",
        "                frame_results = self.analyze_frame_with_crops(frame_path, f\"{file_id}_{frame_index}\")\n",
        "                results[\"frame_results\"].append(frame_results)\n",
        "\n",
        "        # Calculate overall statistics\n",
        "        results[\"statistics\"] = self._calculate_statistics(results[\"frame_results\"])\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _calculate_statistics(self, frame_results):\n",
        "        \"\"\"Calculate overall statistics from frame results.\"\"\"\n",
        "        total_frames = len(frame_results)\n",
        "        fake_frames = sum(1 for f in frame_results if f[\"final_verdict\"] == \"fake\")\n",
        "        total_crops = sum(len(f[\"crop_analyses\"]) for f in frame_results)\n",
        "        fake_crops = sum(\n",
        "            sum(1 for crop in f[\"crop_analyses\"] if crop[\"prediction\"] == \"fake\") for f in frame_results\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"total_frames\": total_frames,\n",
        "            \"fake_frames\": fake_frames,\n",
        "            \"fake_frames_percentage\": ((fake_frames / total_frames * 100) if total_frames > 0 else 0),\n",
        "            \"total_crops\": total_crops,\n",
        "            \"fake_crops\": fake_crops,\n",
        "            \"fake_crops_percentage\": ((fake_crops / total_crops * 100) if total_crops > 0 else 0),\n",
        "        }\n",
        "\n",
        "    def _save_results(self, results, output_dir):\n",
        "        \"\"\"Save analysis results to output directory.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{results['file_id']}_analysis.json\")\n",
        "\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(results, f, sort_keys=True, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-13T19:28:30.678616Z",
          "iopub.status.busy": "2024-12-13T19:28:30.678136Z",
          "iopub.status.idle": "2024-12-13T19:28:34.715046Z",
          "shell.execute_reply": "2024-12-13T19:28:34.713955Z",
          "shell.execute_reply.started": "2024-12-13T19:28:30.678566Z"
        },
        "id": "EDpKSukUenkl",
        "outputId": "34ff0e4b-d827-46e8-b429-9ec130a1bf38",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize pipeline with crops directory\n",
        "pipeline = DeepfakeDetectionPipeline(\n",
        "    frame_model_path=\"/kaggle/working/model/acc99.76_test-2.1_FRAMES_deepfake_detector_resnext50.pth\",\n",
        "    crop_model_path=\"/kaggle/working/model/acc99.53_test-2.1_CROPS_deepfake_detector_resnext50.pth\",\n",
        "    frames_dir=\"/kaggle/working/temp_dataset\",\n",
        "    crops_dir=\"/kaggle/working/temp_face_crops\",\n",
        "    threshold=0.4,\n",
        "    log_level=0,\n",
        "    FRAMES_FILE_FORMAT=\"png\",\n",
        ")\n",
        "\n",
        "# Process media file\n",
        "results = pipeline.process_media(\n",
        "    media_path=\"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_1247.jpg\",\n",
        "    frame_rate=2,\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"File ID: {results['file_id']}\")\n",
        "print(f\"Media type: {results['media_type']}\")\n",
        "print(\"\\nStatistics:\")\n",
        "print(results)\n",
        "# for key, value in results[\"statistics\"].items():\n",
        "#     print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9130b391-9e67-4151-ab41-7794970a0df9",
        "_uuid": "43225108-7c93-4c10-86d6-d0024efdeca6",
        "execution": {
          "iopub.execute_input": "2024-12-13T19:28:34.71738Z",
          "iopub.status.busy": "2024-12-13T19:28:34.717036Z",
          "iopub.status.idle": "2024-12-13T19:28:34.926102Z",
          "shell.execute_reply": "2024-12-13T19:28:34.92491Z",
          "shell.execute_reply.started": "2024-12-13T19:28:34.717348Z"
        },
        "id": "zuzUIiWAenkm",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "597e5c0a-b324-4f35-de1a-5bfa1bb26e15",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Get the list of frames\n",
        "frames = [\n",
        "    os.path.join(\"/kaggle/working/temp_dataset\", f)\n",
        "    for f in os.listdir(\"/kaggle/working/temp_dataset\")\n",
        "    if (f.startswith(\"4567558_11307367\") and (\"ela.png\" in f))\n",
        "][:1]\n",
        "\n",
        "print(\"Found frames:\", frames)\n",
        "\n",
        "# Sort the frames naturally\n",
        "frames = natsorted(frames)\n",
        "\n",
        "if frames:\n",
        "    for frame in frames:\n",
        "        # Read first image\n",
        "        image = cv2.imread(frame)\n",
        "\n",
        "        if image is not None:\n",
        "\n",
        "            plt.imshow(image)\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Could not read image: {frame}\")\n",
        "else:\n",
        "    print(\"No frames found matching the criteria\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9130b391-9e67-4151-ab41-7794970a0df9",
        "_uuid": "43225108-7c93-4c10-86d6-d0024efdeca6",
        "execution": {
          "iopub.execute_input": "2024-12-02T08:47:05.304795Z",
          "iopub.status.busy": "2024-12-02T08:47:05.304249Z",
          "iopub.status.idle": "2024-12-02T08:47:05.31457Z",
          "shell.execute_reply": "2024-12-02T08:47:05.313135Z",
          "shell.execute_reply.started": "2024-12-02T08:47:05.304738Z"
        },
        "id": "BSk6xNqvenkm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(len(os.listdir(\"/kaggle/working/temp_dataset/fake\")))\n",
        "print(len(os.listdir(\"/kaggle/working/temp_dataset/real\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b15811ec-45d0-43bd-b353-da9e5dd7de83",
        "_uuid": "98f3c0eb-54d0-4ee4-b606-78a003a5a5ee",
        "execution": {
          "iopub.execute_input": "2024-10-15T17:03:10.655252Z",
          "iopub.status.busy": "2024-10-15T17:03:10.654822Z",
          "iopub.status.idle": "2024-10-15T17:03:11.767144Z",
          "shell.execute_reply": "2024-10-15T17:03:11.765478Z",
          "shell.execute_reply.started": "2024-10-15T17:03:10.655209Z"
        },
        "id": "ehKbdFo_enkm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!mkdir '/kaggle/working/temp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e199d172-90ae-4d26-ae70-7127b3131d9f",
        "_uuid": "49059c72-44d9-45c6-a29e-8c6e50a69704",
        "execution": {
          "iopub.execute_input": "2024-10-15T17:03:11.770494Z",
          "iopub.status.busy": "2024-10-15T17:03:11.770058Z",
          "iopub.status.idle": "2024-10-15T17:03:11.81333Z",
          "shell.execute_reply": "2024-10-15T17:03:11.811752Z",
          "shell.execute_reply.started": "2024-10-15T17:03:11.770449Z"
        },
        "id": "tJgxjjScenkm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "extract_frames_with_face(\n",
        "    os.listdir(\"/kaggle/input/celeb-df-v2-dataset/Celeb-synthesis\")[-1],\n",
        "    frame_output_dir=\"/kaggle/working/temp\",\n",
        "    id=\"frame\",\n",
        "    frame_rate=1,\n",
        "    log_level=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsR-Uyh5enkm"
      },
      "source": [
        "# New pipeline testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c097021-d44e-4f8f-b4b9-0d35622d6ce4",
        "_uuid": "868f3406-0631-4930-8a83-23a602952cfc",
        "execution": {
          "iopub.execute_input": "2024-12-15T06:54:03.524034Z",
          "iopub.status.busy": "2024-12-15T06:54:03.523619Z",
          "iopub.status.idle": "2024-12-15T06:54:03.746861Z",
          "shell.execute_reply": "2024-12-15T06:54:03.74563Z",
          "shell.execute_reply.started": "2024-12-15T06:54:03.523998Z"
        },
        "id": "NRRUgSiJenkm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DeepfakeDetectionPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        frame_model_path,\n",
        "        crop_model_path,\n",
        "        media_dir,\n",
        "        threshold=0.4,\n",
        "        log_level=0,\n",
        "        FRAMES_FILE_FORMAT=\"jpg\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline with both frame and crop models.\n",
        "\n",
        "        Args:\n",
        "            frame_model_path (str): Path to the frame analysis model\n",
        "            crop_model_path (str): Path to the crop analysis model\n",
        "            media_dir (str): Base directory for media files\n",
        "            threshold (float): Confidence threshold for face detection\n",
        "            log_level (int): Logging verbosity level\n",
        "        \"\"\"\n",
        "        # Load models and move to appropriate device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.frame_model = torch.load(frame_model_path, map_location=self.device)\n",
        "        self.crop_model = torch.load(crop_model_path, map_location=self.device)\n",
        "        self.frame_model.eval()\n",
        "        self.crop_model.eval()\n",
        "\n",
        "        # Set up image transformation\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.log_level = log_level\n",
        "        self.label_map = {0: \"real\", 1: \"fake\"}\n",
        "        self.media_dir = media_dir\n",
        "        self.FRAMES_FILE_FORMAT = FRAMES_FILE_FORMAT\n",
        "\n",
        "        # Initialize MediaProcessor for face detection\n",
        "        self.media_processor = MediaProcessor(\n",
        "            threshold=threshold, log_level=log_level, FRAMES_FILE_FORMAT=self.FRAMES_FILE_FORMAT\n",
        "        )\n",
        "\n",
        "        # Create necessary directories\n",
        "        self.frames_dir = os.path.join(media_dir, \"frames\")\n",
        "        self.crops_dir = os.path.join(media_dir, \"crops\")\n",
        "        self.ela_dir = os.path.join(media_dir, \"ela\")\n",
        "        self.gradcam_dir = os.path.join(media_dir, \"gradcam\")\n",
        "        self.submitted_media_dir = os.path.join(media_dir, \"submitted_media\")\n",
        "\n",
        "        os.makedirs(self.frames_dir, exist_ok=True)\n",
        "        os.makedirs(self.crops_dir, exist_ok=True)\n",
        "        os.makedirs(self.ela_dir, exist_ok=True)\n",
        "        os.makedirs(self.gradcam_dir, exist_ok=True)\n",
        "        os.makedirs(self.submitted_media_dir, exist_ok=True)\n",
        "\n",
        "    def get_crops_for_frame(self, file_id, frame_index, crops_dir):\n",
        "        \"\"\"\n",
        "        Get all crops belonging to a specific frame using the naming scheme.\n",
        "\n",
        "        Args:\n",
        "            file_id (str): Combined hash identifier (content_hash + name_hash)\n",
        "            frame_index (int): Frame index (0 for images)\n",
        "            crops_dir (str): Directory containing all crops\n",
        "\n",
        "        Returns:\n",
        "            list: Paths to relevant crop files\n",
        "        \"\"\"\n",
        "        crop_prefix = f\"{file_id}_{frame_index}_\"\n",
        "        relevant_crops = []\n",
        "\n",
        "        for filename in os.listdir(crops_dir):\n",
        "            if filename.startswith(crop_prefix):\n",
        "                crop_path = os.path.join(crops_dir, filename)\n",
        "                relevant_crops.append(crop_path)\n",
        "\n",
        "        return natsorted(relevant_crops)  # Sort to ensure consistent ordering\n",
        "\n",
        "    def load_image_preprocessed(self, image_path, show_image=False):\n",
        "        if show_image:\n",
        "            cv_img = cv2.imread(image_path)\n",
        "\n",
        "            # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "            cv_img_rgb = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Display the image with detected faces\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(cv_img_rgb)\n",
        "            plt.axis(\"off\")  # Hide axis\n",
        "            plt.title(f\"Test image\")\n",
        "            plt.show()\n",
        "\n",
        "        #  Define the transformations (should be the same as used in training)\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),  # Resize the image to the input size of the model\n",
        "                transforms.ToTensor(),  # Convert image to tensor\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),  # Normalize as per the pre-trained model's requirements\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply the transformations\n",
        "        image = transform(image)\n",
        "\n",
        "        # Add a batch dimension (models expect a batch of images, even if it's just one image)\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def process_frame(self, image_path, type=\"frame\"):\n",
        "        \"\"\"\n",
        "        Process a single frame through frame-level model with integrated GradCAM for frames only.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "            type (str): Type of processing, either \"frame\" or \"crop\"\n",
        "\n",
        "        Returns:\n",
        "            tuple: (predicted_label, confidence_score, gradcam_path if type==\"frame\" else None)\n",
        "        \"\"\"\n",
        "        if type not in [\"frame\", \"crop\"]:\n",
        "            raise ValueError(\"Invalid type. Expected 'frame' or 'crop'.\")\n",
        "\n",
        "        model = self.frame_model if type == \"frame\" else self.crop_model\n",
        "        gradcam_path = None\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = self.load_image_preprocessed(image_path, show_image=False)\n",
        "        image = image.to(self.device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            probabilities = tnf.softmax(output, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "            predicted_class = predicted.item()\n",
        "\n",
        "        predicted_label = self.label_map[predicted_class]\n",
        "        confidence_score = confidence.item()\n",
        "\n",
        "        # Generate GradCAM only for frame-level analysis\n",
        "        if type == \"frame\":\n",
        "            gradcam_path = os.path.join(\n",
        "                self.gradcam_dir,\n",
        "                os.path.basename(image_path).replace(\n",
        "                    f\".{self.FRAMES_FILE_FORMAT}\", f\"_gradcam.{self.FRAMES_FILE_FORMAT}\"\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # Target the last convolutional layer\n",
        "            target_layers = [model.layer4[-1]]\n",
        "\n",
        "            # Create GradCAM object\n",
        "            cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "            # Define target for GradCAM\n",
        "            targets = [ClassifierOutputTarget(predicted_class)]\n",
        "\n",
        "            # Generate grayscale CAM\n",
        "            grayscale_cam = cam(input_tensor=image, targets=targets)\n",
        "            grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "            # Load and prepare original image for overlay\n",
        "            rgb_img = cv2.imread(image_path)\n",
        "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
        "            # rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB) #convert BGR to RGB color space\n",
        "            rgb_img = rgb_img / 255.0\n",
        "\n",
        "            # Create visualization\n",
        "            visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "            # visualization = cv2.cvtColor(visualization, cv2.COLOR_BGR2RGB) #convert BGR to RGB color space\n",
        "\n",
        "            # plt.imshow(visualization)\n",
        "\n",
        "            # Save visualization\n",
        "            cv2.imwrite(gradcam_path, visualization)\n",
        "\n",
        "            if self.log_level >= 2:\n",
        "                print(f\"GradCAM saved to: {gradcam_path}\")\n",
        "\n",
        "        if self.log_level >= 2:\n",
        "            print(f\"Type: {type}\")\n",
        "            print(f\"Predicted label: {predicted_label}\")\n",
        "            print(f\"Confidence score: {confidence_score*100:.2f}\")\n",
        "\n",
        "        return predicted_label, confidence_score, gradcam_path if type == \"frame\" else None\n",
        "\n",
        "    def analyze_frame_with_crops(self, image_path, frame_id):\n",
        "        \"\"\"\n",
        "        Analyze a frame both at frame-level and crop-level.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "            frame_id (str): Identifier for the frame\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results including frame and crop predictions\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"frame_id\": frame_id,\n",
        "            \"frame_analysis\": None,\n",
        "            \"crop_analyses\": [],\n",
        "            \"final_verdict\": None,\n",
        "            \"frame_path\": image_path,\n",
        "            \"crop_paths\": [],\n",
        "            \"ela_path\": None,\n",
        "            \"gradcam_path\": None,\n",
        "        }\n",
        "\n",
        "        # Frame-level analysis with GradCAM\n",
        "        frame_pred, frame_conf, gradcam_path = self.process_frame(image_path, type=\"frame\")\n",
        "        results[\"frame_analysis\"] = {\"prediction\": frame_pred, \"confidence\": frame_conf}\n",
        "        results[\"gradcam_path\"] = gradcam_path\n",
        "\n",
        "        # Get crops for this frame\n",
        "        frame_index = 0 if \"_\" not in frame_id else int(frame_id.split(\"_\")[-1])\n",
        "        file_id = frame_id.rsplit(\"_\", 1)[0]\n",
        "        crop_paths = self.get_crops_for_frame(file_id, frame_index, self.crops_dir)\n",
        "\n",
        "        # Analyze each crop (without GradCAM)\n",
        "        for crop_path in crop_paths:\n",
        "            crop_pred, crop_conf, _ = self.process_frame(crop_path, type=\"crop\")\n",
        "\n",
        "            crop_index = int(os.path.splitext(crop_path)[0].split(\"_\")[-1])\n",
        "\n",
        "            results[\"crop_analyses\"].append(\n",
        "                {\n",
        "                    \"face_index\": crop_index,\n",
        "                    \"prediction\": crop_pred,\n",
        "                    \"confidence\": crop_conf,\n",
        "                }\n",
        "            )\n",
        "            results[\"crop_paths\"].append(crop_path)\n",
        "\n",
        "        # Determine final verdict\n",
        "        if len(results[\"crop_analyses\"]) > 0:\n",
        "            frame_is_fake = frame_pred == \"fake\"\n",
        "            crops_fake_count = sum(\n",
        "                1 for crop in results[\"crop_analyses\"] if crop[\"prediction\"] == \"fake\"\n",
        "            )\n",
        "            crops_total = len(results[\"crop_analyses\"])\n",
        "\n",
        "            if frame_is_fake and crops_fake_count > crops_total / 2:\n",
        "                results[\"final_verdict\"] = \"fake\"\n",
        "            else:\n",
        "                results[\"final_verdict\"] = \"real\"\n",
        "        else:\n",
        "            results[\"final_verdict\"] = frame_pred\n",
        "\n",
        "        # Perform ELA analysis\n",
        "        results[\"ela_path\"] = self.perform_ela_analysis(image_path)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def perform_ela_analysis(self, image_path):\n",
        "        \"\"\"\n",
        "        Perform Error Level Analysis (ELA) on the image.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the ELA image\n",
        "        \"\"\"\n",
        "        ela_image_path = os.path.join(\n",
        "            self.ela_dir,\n",
        "            os.path.basename(image_path).replace(\n",
        "                f\".{self.FRAMES_FILE_FORMAT}\", f\"_ela.{self.FRAMES_FILE_FORMAT}\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # # Skip if ELA image already exists\n",
        "        # if os.path.exists(ela_image_path):\n",
        "        #     if self.log_level >= 1:\n",
        "        #         print(f\"Skipping frame {ela_image_path}: already exists\")\n",
        "        #     return ela_image_path\n",
        "\n",
        "        # Open image using PIL\n",
        "        original_image = Image.open(image_path)\n",
        "        temp_compressed = os.path.join(os.path.dirname(image_path), \"temp_compressed.jpg\")\n",
        "\n",
        "        # Save compressed version\n",
        "        quality = 95\n",
        "        scale_multiplier = 50\n",
        "        original_image.save(temp_compressed, \"JPEG\", quality=quality)\n",
        "        compressed_image = Image.open(temp_compressed)\n",
        "\n",
        "        # Calculate difference\n",
        "        ela_image = ImageChops.difference(original_image, compressed_image)\n",
        "\n",
        "        # Apply noise boost and scaling\n",
        "        ela_scaled = ela_image.point(lambda x: np.sign(x) * (np.abs(x) ** 1.5 * scale_multiplier))\n",
        "\n",
        "        # Save ELA image\n",
        "        ela_scaled.save(ela_image_path)\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_compressed):\n",
        "            os.remove(temp_compressed)\n",
        "\n",
        "        return ela_image_path\n",
        "\n",
        "    def process_media(self, media_path, frame_rate=2):\n",
        "        \"\"\"\n",
        "        Process a media file (image or video) through the pipeline.\n",
        "\n",
        "        Args:\n",
        "            media_path (str): Path to the media file\n",
        "            frame_rate (int): Frame rate for video processing\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results for the entire media file\n",
        "        \"\"\"\n",
        "        media_type = self.media_processor.check_media_type(media_path)\n",
        "\n",
        "        # Generate file identifier using MediaProcessor's hash functions\n",
        "        file_id = self.media_processor.generate_combined_hash(media_path)\n",
        "\n",
        "        # Process media file for frames using MediaProcessor\n",
        "        self.media_processor.process_media_file(\n",
        "            media_path, self.frames_dir, generate_crops_flag=False, frame_rate=frame_rate\n",
        "        )\n",
        "\n",
        "        # Process media file for crops using MediaProcessor\n",
        "        self.media_processor.process_media_file(\n",
        "            media_path, self.crops_dir, generate_crops_flag=True, frame_rate=frame_rate\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            \"media_path\": media_path,\n",
        "            \"media_type\": media_type,\n",
        "            \"file_id\": file_id,\n",
        "            \"frame_results\": [],\n",
        "        }\n",
        "\n",
        "        if media_type == \"Image\":\n",
        "            media_path = os.path.join(self.frames_dir, f\"{file_id}_0.{self.FRAMES_FILE_FORMAT}\")\n",
        "            frame_results = self.analyze_frame_with_crops(media_path, f\"{file_id}_0\")\n",
        "            results[\"media_path\"] = media_path\n",
        "            results[\"frame_results\"].append(frame_results)\n",
        "\n",
        "        elif media_type == \"Video\":\n",
        "            frames = [\n",
        "                os.path.join(self.frames_dir, f)\n",
        "                for f in os.listdir(self.frames_dir)\n",
        "                if (f.startswith(file_id) and (\"ela\" not in f and \"gradcam\" not in f))\n",
        "            ]\n",
        "            frames = natsorted(frames)\n",
        "            for frame_index, frame_path in enumerate(frames):\n",
        "                # print(\"ID/PATH: \",frame_index, frame_path)\n",
        "                frame_results = self.analyze_frame_with_crops(frame_path, f\"{file_id}_{frame_index}\")\n",
        "                results[\"frame_results\"].append(frame_results)\n",
        "\n",
        "        # Calculate overall statistics\n",
        "        results[\"statistics\"] = self._calculate_statistics(results[\"frame_results\"])\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _calculate_statistics(self, frame_results):\n",
        "        \"\"\"Calculate overall statistics from frame results.\"\"\"\n",
        "        total_frames = len(frame_results)\n",
        "        fake_frames = sum(1 for f in frame_results if f[\"final_verdict\"] == \"fake\")\n",
        "        total_crops = sum(len(f[\"crop_analyses\"]) for f in frame_results)\n",
        "        fake_crops = sum(\n",
        "            sum(1 for crop in f[\"crop_analyses\"] if crop[\"prediction\"] == \"fake\") for f in frame_results\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"total_frames\": total_frames,\n",
        "            \"fake_frames\": fake_frames,\n",
        "            \"fake_frames_percentage\": ((fake_frames / total_frames * 100) if total_frames > 0 else 0),\n",
        "            \"total_crops\": total_crops,\n",
        "            \"fake_crops\": fake_crops,\n",
        "            \"fake_crops_percentage\": ((fake_crops / total_crops * 100) if total_crops > 0 else 0),\n",
        "        }\n",
        "\n",
        "    def _save_results(self, results, output_dir):\n",
        "        \"\"\"Save analysis results to output directory.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{results['file_id']}_analysis.json\")\n",
        "\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(results, f, sort_keys=True, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-15T06:57:21.706213Z",
          "iopub.status.busy": "2024-12-15T06:57:21.705163Z",
          "iopub.status.idle": "2024-12-15T06:57:21.987294Z",
          "shell.execute_reply": "2024-12-15T06:57:21.986237Z",
          "shell.execute_reply.started": "2024-12-15T06:57:21.706166Z"
        },
        "id": "srj4bpuhenkn",
        "outputId": "95beb710-e0cd-4308-e8d3-4b9f1da61199",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize pipeline with media directory\n",
        "pipeline = DeepfakeDetectionPipeline(\n",
        "    frame_model_path=\"/kaggle/working/model/acc99.76_test-2.1_FRAMES_deepfake_detector_resnext50.pth\",\n",
        "    crop_model_path=\"/kaggle/working/model/acc99.53_test-2.1_CROPS_deepfake_detector_resnext50.pth\",\n",
        "    media_dir=\"/kaggle/working/media\",\n",
        "    threshold=0.4,\n",
        "    log_level=0,\n",
        "    FRAMES_FILE_FORMAT=\"png\",\n",
        ")\n",
        "\n",
        "# Process media file\n",
        "results = pipeline.process_media(\n",
        "    media_path=\"/kaggle/input/test-images-for-face-detection-pipeline-samples/test_images_for_face_detection_pipeline/img_1252.jpg\",\n",
        "    frame_rate=2,\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"File ID: {results['file_id']}\")\n",
        "print(f\"Media type: {results['media_type']}\")\n",
        "print(\"\\nStatistics:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idx2Qdyuenkn",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DF_detectection-ResNext-FYP-P1.ipynb",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5635361,
          "sourceId": 9306137,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5636749,
          "sourceId": 9333436,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "DMI_FYP_dj_primary-backend-cvNQsyiC",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
